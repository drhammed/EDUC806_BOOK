---
title: "EDUC_806- Quantitative Research Methods"
author: "Hammed Akande"
project:
  type: website
  output-dir: docs

date: 16 October 2023
---

# Chapter 4- Regression

### Simple Linear Regression (SLR)

During Tutorial 2 & 3 ("Model Terminology"), I explained that Linear model and Linear regression are just synonyms and we often use either terms when quantifying the effect of a "**continuous"** independent variable on a"**continuous"** dependent (response) variable. The difference between this and ANOVA is that **ANOVA** is usually used when quantifying the effect of a "**discrete (or categorical)"** independent variable on a"**continuous"** dependent variable. So, it is important to note that- *ANOVA is also a linear regression!* In fact, if you run "anova" function on linear model object, you'll most likely get the same p-value.

**Regression** generally refers to the fact that we are quantifying the relationship between a response variable and (one or more) predictor variables. In the case of **SLR**, both the response and the predictor are *numeric* variables and we are using a single predictor (independent) variable. Later we will use multiple predictor variables (multiple regression). Also, this models tells us that our model for Y is a linear combination of the predictors X. (In this case just one predictor)! For now, this always results in a model that is a line, but this is not always the case (and we may see this later on in the semester).

Like ANOVA, in SLR, we often talk about the assumptions that this model makes. This include-

1.  Linearity- the relationship between Y and x is linear, of the form $\beta_0 + \beta_1x$.
2.  Independent. The errors $\epsilon$ are independent.
3.  Normality. The errors, $\epsilon$ are normally distributed. I.e. the "error" around the line follows a normal distribution.
4.  Equality of Variance. At each value of x, the variance of Y is the same.

For this lab, we will be using a year dataset on Corvettes sales in Virginia Beach, Virginia. Using this data, ten Corvettes (between 1 and 6yrs old) were randomly selected and the below data shows the sales price (in hundreds of dollars) denoted by y and the age (in years) denoted by x.

![](images/Corvettes-01.PNG){fig-align="center"}

#### Tasks

1.  Graph the data in a scatterplot to determine if there is a possible linear relationship.
2.  Compute and interpret the linear correlation coefficient, r.
3.  Determine the regression equation for the data.
4.  Graph the regression equation and the data points.
5.  Identify outliers and potential influential observations.
6.  Compute and interpret the coefficient of determination, r2.
7.  Obtain the residuals and create a residual plot. Decide whether it is reasonable to consider that the assumptions for regression analysis are met by the variables in questions.
8.  At the 5% significance level, do the data provide sufficient evidence to conclude that the slope of the population regression line is not 0 and, hence, that age is useful as a predictor of sales price for Corvettes?
9.  Obtain and interpret a 95% confidence interval for the slope, β, of the population regression line that relates age to sales price for Corvettes.
10. Obtain a point estimate for the mean sales price of all 4-year-old Corvettes.
11. Determine a 95% confidence interval for the mean sales price of all 4-year-old Corvettes.
12. Find the predicted sales price of Jack Smith's 4-year-old Corvette.
13. Determine a 95% prediction interval for the sales price of Jack Smith's 4-year-old Corvette.

This is the [link](https://drive.google.com/drive/folders/1nb1zgxr_IIV9271-BWmffmMSukOYf3ja?usp=sharing) to the main google drive folder for all the data to complete Lab_5!

To begin, open your SPSS and load in the Corvettes_data. I've reprocessed it and you can download it [here](https://drive.google.com/file/d/1Hk_w8GMsFSajXT4sj57zhzax1oCm1zew/view?usp=drive_link). if for anything that doesn't work, you can manually enter the two variables in the SPSS.

![](images/Corv_data.PNG){fig-align="center"}

*1. To graph the data in a scatterplot and determine if there is a possible linear relationship*.

Select Graphs \> Scatter/Dot \> select Simple, then click the Define button. The Y-axis should be the Price and the X Axis variable = Age.  Click on "Titles" and you can enter a descriptive title for your graph, and click "Continue." Click "OK".

*Your graph should look similar to the figure below*

![](images/Price%20vs%20Age.PNG){fig-align="center"}

Interpretation- **The points seem to follow a linear pattern, although with a negative slope**.

2.  To compute the linear correlation coefficient,

    Click on "Analyze" \> "Correlate" \> "Bivariate". Select "Age" and "Price" as the variables and select "Pearson" as the correlation coefficient. Finally, you can run the analysis.

    ![](images/correlation_output.PNG)

    The Pearson Correlation is a statistical method that calculates the strength and direction of linear relationships between continuous variables. It produces a sample correlation coefficient, r, which can be used to evaluate whether there is a linear relationship among the same variables in the population. This population correlation coefficient is represented by ρ ("rho") and is a parametric measure.

    Pearson correlation indicates:

    -   Whether there is a statistically significant linear relationship between two continuous variables

    -   It also shows the strength of this linear relationship (i.e., how close the relationship is to being a perfectly straight line)

    -   Finally, it reveals the direction of this linear relationship (increasing or decreasing)

    It is however important to note that Pearson Correlation cannot address non-linear relationships or relationships among categorical variables. To address relationships that involve categorical variables and/or non-linear relationships, you need to consider the equivalent non-parametric test (e.g., Spearman's rank correlation).

    Also, while Pearson Correlation reveals *associations* among (continuous) variables, you should remember that "*Correlation* *does not imply causation,*" no matter how large the correlation coefficient is.

    **Interpretation**- The correlation coefficient is **-0.968**. This r-value indicates a robust negative linear correlation, given its proximity to -1 and negative sign. This strong negative linear correlation suggests that data points should closely cluster around a downward-sloping regression line, (*which aligns with the graph above*). Consequently, the presence of a strong negative linear relationship between Age and Price supports the continuation of linear regression analysis.

3.  Regression equation for the data.

    Because our goal here is to predict the price of 4-year-old Corvettes, let's modify the data variable before we proceed. Under the "Age" variable, enter the number "4" after the last row. Also, at the last row of the "Price" variable, enter a dot "." This way, we're telling SPSS that we want a prediction for this value and not to include the value in any other computations.

    ![](images/Cor_pred_data.PNG)

    Select Analyze \> Regression \> Linear. Select "Price" as the dependent variable and "Age" as the independent variable. Click "Statistics," select "Estimates" and "Confidence Intervals" for the regression coefficients, select "Model fit" to obtain r2, and click "Continue."

    Click "Plots," select "Normal Probability Plot" of the residuals and click "Continue." Click "Save," select "Unstandardized" predicted values, select "Unstandardized" and "Studentized" residuals, select "Mean" (to obtain a confidence interval output in the Data Window) and "Individual" (to obtain a prediction interval output in the Data Window) at the 95% level, and click "Continue."  Finally, click "OK" to run the analysis.

    ![](images/regression_equation_output.PNG)

    From above, the regression equation is: Price = 29160.194 -- (2790.291)(Age). So what if a newly sold Corvettes was 10years old? What would be the price? *Y* = 29160.194 -- (2790.291)(10) which equals \~ \$1257.284.

4.  Graph the regression equation and the data points

    Another way to visualize our regression line is through a scatterplot. Click on **Graphs**, then **Scatter/Dot**. In the Scatter/Dot dialog box that appears, be sure "Simple Scatter" is selected and then click **Define**. Move "Price" to the "Y Axis" box and move "Age" to the "X Axis" box and then click **OK**. A simple scatterplot should appear in your Output Viewer window. Double click the graph to show the "Chart Editor" window. Now, click **Elements** and then click **Fit Line at Total**. Now your scatterplot displays the linear regression line computed above while also providing you with the regression equation and the R squared value. This is a good way to double-check that we've written our equation correctly.

    ![](images/regression_plot.PNG)

5.  Identify the Outliers and potential influential observations

    From the plot, there seem to be no points that lie far from the cluster of data points or far from the regression line; thus, no possible outliers.

6.  Compute and interpret the coefficient of determination, r^2^.

    ![](images/model_summary_r2.PNG)

    The r^2^ = 0.937; therefore, about 93.7% of the variation in the price data is explained by age. I.e., as the car gets older, the value/price drops!  The regression equation appears to be very useful for making predictions since the value of r^2^ is close to 1.

    Note- we also have adjusted R-square. R-square technically measures the variation of a regression model (variation in Y given x). R-squared either increases or remains the same when new predictors are added to the model. Adjusted R-squared measures the variation for a multiple regression model, and helps you determine goodness of fit. For the purpose of this SLR (one predictor, we are not adding more), so we can decide to intepret any of them. But if you have multiple predictors, you may want to look into Adj. R2)!

7.  Residuals

    The residuals, predicted values and confidence intervals can be found in the data window.

    ![](images/Residuals-02.PNG)

    To create a residual plot, select Graphs\> Scatter/Dot\>Simple. Select  the residuals (RES_1) as the Y Axis variable and Age as the X Axis variable. Click "Titles" to enter "Residual Plot" as the title for your graph, and click "Continue". Click "OK".

    Double-click the resulting graph in the output window, select "Options" \> "Y Axis Reference Line", select the "Reference Line" tab in the properties window, add position of line "0", and click "Apply". Click the close box to exit the chart editor.

    ![](images/Residual_plot-02.PNG)

    Do the same scatter plot to create a studentized residual plot. Select the SRES as the Y axis in this case (Age remains the X axis). Set the title and enter "Studentized Residual Plot".

    ![](images/studentized_regression_plot-02.PNG)

    **Also, to assess the normality of the residuals, consult the P-P Plot from the regression output.**

    ![](images/normal_p-p_plot-02.PNG)

    **Interpretation**- The residual plot shows a random scatter of the points (independence) with a constant spread (variance). Also, the studentized residual plot shows a random scatter of the points (independence) with a constant variance and with no values beyond the ±2 standard deviation reference lines (no outliers). The normal probability plot of the residuals shows the points close to a diagonal line; therefore, the residuals appear to be approximately normally distributed. Thus, we have evidence to conclude that the assumptions for regression analysis appear to be met.

8.  Question - *Using 5% significance level, do the data provide sufficient evidence to conclude that the*

    *slope of the population regression line is not 0 and, hence, that age is useful as a predictor*

    *of sales price for Corvettes*?

    Here, we need to state the hypothesis. The hypothesis from the question above is that-

    $H_0$: $\beta = 0$ (Age is not a useful predictor of price.)

    $H_a$: $\beta \neq 0$ (Age is a useful predictor of price.)

    Now that we've done that, remember our alpha = 0.05 and our critical value or rejection region is that we should reject the null hypothesis if alpha is less than 0.05. Finally, to answer the question, you can choose your test Statistic (choose either the T-test method or the F-test method- not both!). Check the regression output and see the "coefficient". From the result of the regression, you can see that T = --10.887, and p-value = \<0.001

    ![](images/regression_coeff.PNG)

    **Interpretation**- Since the P-value \< 0.05, we have evidence to reject the null hypothesis. In other words, we have enough evidence to conclude that the slope of the population regression line is not zero. In other words, age is a useful predictor of price for Corvettes.

9.  *Obtain and interpret a 95% confidence interval for the slope, β , of the population regression line that relates age to sales price for Corvettes.*

    Look at the regression coefficient above, We are 95% confident that the slope of the true regression line is somewhere between --3381.295 and --2199.288. In other words, we are 95% confident that for every year older Corvettes get, their average price decreases somewhere between \$3,381.295 and \$2,199.288.

10. *Obtain a point estimate for the mean sales price of all 4-year-old Corvettes.*

    Look at the data window (last row), the point estimate (PRE_1) is 17999.02913 dollars (\$17,999.02913).

11. *Determine a 95% confidence interval for the mean sales price of all 4-year-old Corvettes.*

    Still on the last row of the data table, we are 95% confident that the mean sales price of all four-year-old Corvettes is somewhere between \$16,958.46042 (LMCI_1) and \$19,039.59783 (UMCI_1).

12. *Find the predicted sales price of Jack Smith's selected 4-year-old Corvette*.

    The predicted sales price is 17999.02913 dollars (\$17,999.02913).

13. *Determine a 95% prediction interval for the sales price of Jack Smith's 4-year-old*

    *Corvette.*

    We are 95% certain that the individual sales price of Jack Smithʼs Corvette will be

    somewhere between \$14,552.91726 (LICI_1) and \$21,445.14099 (UICI_1).

### Multiple Linear Regression

So far, we have learned how to calculate a linear regression equation and make predictions. However, what if there are other potential independent variables to consider? In fact, It is uncommon for a dataset or research study to have only one predictor. Similarly, it is rare for a response variable to depend solely on a single variable. In this chapter, we will expand our simple linear regression (SLR) model to include multiple predictors. Multiple regression allows us to incorporate multiple independent variables and assign weights to each of them, resulting in more accurate predictions. The process of conducting a multiple regression in SPSS is similar to that of linear regression, with the difference being the inclusion of additional independent variables.

For this lab, we shall be using the "autompg" dataset containing car information. This dataset provides data on fuel economy from 1999 and 2008 for about 38 popular models of cars. It contains a response variable known as "mpg," which records the city fuel efficiency of cars, alongside several predictor variables detailing the vehicle attributes. You can download the data from our google drive folder. See the link [here](https://drive.google.com/file/d/1Hk_w8GMsFSajXT4sj57zhzax1oCm1zew/view?usp=sharing). For anyone using R, you can find the dataset loaded with the *ggplot2* package.

*Brief description of the variables in the data*

| Variable | Type    | Description                   |
|:---------|:--------|:------------------------------|
| mpg      | numeric | city fuel efficiency          |
| cyl      | integer | number of cylinders           |
| displ    | numeric | engine displacement in liters |
| hp       | numeric | horse power                   |
| wt       | numeric | Weight                        |
| acc      | numeric | acceleration                  |
| year     | integer | year of manufacturing         |

At this point, our focus will be on utilizing two variables, "wt" and "year," as predictor variables. In other words, we aim to create a model that predicts a car's fuel efficiency (mpg) based on its weight (wt) and the model year (year). To achieve this, we will formulate the following linear model:

$$ Y_i = \beta_i + \beta_1X_1 + \beta_2X_2 + \epsilon_i, \hspace4ex i = 1, 2, …,n$$

where $\epsilon_i \sim N(0,\alpha^2)$ , $x_{i1}$ = the weight (wt) of the $i$ car, and $x_{i2}$ as the model year (year) of the $i$ car.

**Task- before we do for multiple predictors, let's quickly revist SLR and use one predictor. For this part, perform a simple linear regression of mpg against wt. What's the R-squared? is weight a good predictor of mpg? (refer back to the tutorial on SLR- we just covered all of that)!**

Your model summary should be like below.

![](images/model_summary_mpg_vs_wt-01.PNG){fig-align="center"}

*Okay, now that we have done that- we can proceed to multiple linear regression. Recall, we want to build a model with mpg as dependent variable, while wt and year as independent variables.*

To do this SPSS,

1.  Click on **Analyze**, then **Regression**, then **Linear** from the submenu to open the Linear Regression dialog.

2.  In the Linear Regression dialog:

    -   Move the dependent variable "mpg" to the "Dependent" box.

    -   Move the independent variables "wt" and "year" to the "Independent(s)" box.

3.  Click the "Statistics" button to access the statistics options and check "estimates" (under regression coefficient) and model fit to request the coefficients of the linear model and model fit.

4.  Click "Continue" to return to the Linear Regression dialog.

5.  Finally, click "OK" to run the linear regression analysis.

**Questions**

1\) Looking at the Output Viewer window, you should sea tables that are similar to the ones in our previous example (our model summary is new). Take a look at the "model summary" to determine the new R squared and Standard Error of the Estimate. Compare that with the previous regression output- ***has the inclusion of additional variables (year) resulted in an improvement in our model***?

![](images/model_summary_wt_year_mpg.PNG){fig-align="center"}

Note- *the new R squared has improved, and the Standard Error has reduced! This indicates that our multiple regression model is more precise than the previous linear regression model.*

2\) ***Do all our variables hold statistical significance***?

You can verify this by checking the ANOVA table. The F statistic obtained and its significance indicate whether our independent variables have a statistically significant association with our dependent variable. It also helps us determine whether our model is a good fit for explaining the variation in the mpg.

In our case, we have evidence to believe it is, given that the significance (sig) is way below 0.05 (\<0.001).

Lastly, check the "Coefficients" table. Here we find the value of *a* (or the slope) for each of our independent variables (wt and year) and we also find our intercept.

![](images/Coefficient_mlr.PNG){fig-align="center"}

So, how can we write our multiple regression equation? mpg (*Y*) = -14.638 -- 0.07(weight) + 0.761(year). Thus, if we added a new car and had some basic data like the year and weight, we would be able to estimate, with a relatively high degree of confidence, how many mpg would be used given the predictors.

**Interpretation**

Here, the constant = -14.638 represents our estimate for the intercept, i.e.- the mean miles per gallon for a car that has a weight of 0 pounds and was manufactured in 1900 (the start year of our dataset). As we can see here that the estimate is negative, which, in the real world, is physically impossible. However, this is not surprising because we cannot realistically expect our model to accurately predict the fuel efficiency of cars from 1900 that weigh 0 pounds because such vehicles never existed anyways! So, like simple linear regression, this value (intercept) represent the mean of Y when all predictors are set to 0.

However, the interpretation of the coefficients of our predictors is slightly different from previous SLR. For instance, the estimate of -0.007 for "wt" = the expected average change in miles per gallon for a one-unit increase in weight for cars of a specific model year, with the year being held constant. Note that this estimate is negative, which aligns with our expectations, as, in general, fuel efficiency tends to decrease for larger vehicles. However, in the context of multiple linear regression, this interpretation is contingent upon a fixed value for another predictor, such as "year" in our case. This means that the relationship between fuel efficiency and weight might not hold true when additional factors, like the model year, are taken into account, potentially causing a reversal in the sign of our coefficient.

Lastly, the estimate of 0.761 for "year" = the expected average change in miles per gallon for a one-year increase in the model year for cars with a specific weight, where weight is held constant now. It is not far from expectation that this estimate is positive since one would anticipate that, over time, as technology advances, cars with the specific weight would achieve better fuel efficiency compared to their earlier counterparts.

Note- Sometimes, you may discover that the model is not statistically significant, or that one independent variable does not hold statistical significance. In such instances, you may want to rerun the model, eliminating insignificant or redundant variables. Ideally, it is good to do some variable importance selection on your predictors before including them in the model (or use some prior knowledge of the system). It may take several attempts to run multiple regression models to find the best-fitting model for the data. Generally, it is good to have model with a low standard error of the estimate, high R squared and relatively simple. A model with three independent variables, a relatively high R squared and low standard error may be preferable to a model with 19 independent variables and a high R squared and low standard error (this is why variable importance is crucial)!

To do variable importance selection, you can calculate the multicollinearity. In SPSS, you can compute the Variance Inflation Factor (VIF). This can be done under regression\> statistics\> collinearity. The output should be like below.

![](images/Coefficient_mlr_vif.PNG){fig-align="center"}

The VIF values indicate the degree of multicollinearity for each variable in the model. In our case, the VIF (for both wt and year) is around 1.104. Ideally, a VIF of 1 means that variables are not correlated and no multicollinearity in the regression model. Generally, a VIF \>6 is considered a sign of high multicollinearity between the predictor variables and can affect the stability and interpretability of your regression model. You may need to address multicollinearity by either removing one of the correlated variables (redundant) or you can use dimensionality reduction techniques like Principal Component Analysis (PCA) to reduce dimensions (we shall come back to PCA soon).

#End

# Chapter 5- Regression Contd.

### Least Square Approach

last week, we talked about simple linear regression and its assumption- linearity, independent, normally distributed and homogeneity of variance. We also mentioned that simple linear regression models the dependent variable Y as a a **linear** function of independent variable (or variables in the case of multiple predictors) X. This means we would expect a plot similar to the one below (you can use this [dataset](https://drive.google.com/file/d/1cMBHj5uNH_ZsPSa04N6AjWmBcIrbvXJJ/view?usp=sharing) to reproduce the plot (*make a scatter plot of weight vs height*).

![](images/student_ht_wt-01.PNG){fig-align="center"}

Looking at the plot, do you think this line best summarizes the trend between the students height and weight?

Before we proceed, let's introduce equation of the best fitting line.

$\hat{y} = \beta_0 + \beta_1 x_1$

where

-   $\hat{y}_i$ is the predicted response (or fitted value) for experimental unit *i*

-   $y_i$ denotes the observed response for experimental unit *i*

-   $x_i$ denotes the predictor value for experimental unit *i*

Now, let's apply this formula on the plot above. Remember from the plot above, our regression equation is $\hat{y}$ = -267 + 6.14x or $\hat{weight}$ = -267 + 6.14 height

The first data on the plot shows that student 1 has a height of 63 inches and a weight of around 127 pounds (i.e. $x_1$ = 63 and $y_1$ = 127). Assuming we know this student's height but not weight, we could use the equation of the line to predict the student's weight. Thus, we'd predict the student's weight to be -267 + 6.14(63) or 119.82 pounds. Which means our predicted y ($\hat{y}$) = 119.82 pounds. Apparently, this is not the same as can be seen on the plot ($y_1$ = 127) and this means we have **prediction** or **residual error**. As a matter of fact, the residual error can be calculated as 127-119.82= 7.18 pounds. You can do the same for all values of x (heights) and get your observed (y) responses, your predicted responses and residual errors. In summary, when we use the equation $\hat{y} = \beta_0 + \beta_1 x_1$ to make prediction of the actual response of $y_i$, we make prediction or residual error $e_i = y_i - \hat{y}$ (which means that the size of the residual error depends on the data point).

An important question, however, is "What is the best fitting line or how do we define a good line"? There are several lines we could use, and our aim is to identify one that is characterized by "minimal errors." In other words, a line that fits the data "**best**" will be one where the ***n*** **prediction errors, ---** each corresponding to an observed data point **--- are minimized overall**. The next question is then, how do we identify such a line? There are numerous methods we could employ for this purpose. One way to achieve this is by applying the "**least squares approach**" which means we need to find the line that "minimizes the sum of squared prediction errors. In other words, we need to find the values of $b_0$ and $b_1$ that can make the sum of the squared residual errors as minimal as possible.

Mathematically, we need to find the values of $b_0$ and $b_1$ that minimizes

$$\text Q = \sum^{n}_{i==1} (y_i - \hat{y})^2$$

Where our

-   prediction (residual) error for data point *i* = $y_i - \hat{y}_i$

-   the squared prediction (residual) error for data point *i* = $(y_i - \hat{y}_i)^2$

-   Lastly, the summation symbol indicate that we should add up the squared prediction (residual) errors for all *n* data points.

So, to summarize, imagine you fit two lines to the data and you want to know which one best describe the trend, you'd pick the line with the lowest sum of squared residual (prediction) error as the **best line**.

From the plot above, note that that if we (manually) adopt the least square method described above to find the equation of the line that minimizes the sum of squared residual error, we may encounter big issues. Specifically, we would need to execute this procedure for an infinite variety of potential lines (not convenient!). Luckily, someone has already done the laborious work of deducing formulas for both the intercept and slope in the equation of the line that minimizes the sum of squared residual error (derived using Calculus).

Here, we minimize the equation for the sum of the squared residual errors:

![](images/LS_formula1.PNG){fig-align="center"}

thus, we can get our least squares estiates for $b_0$ and $b_1$ by taking the derivative with respect to $b_0$ and $b_1$, set to 0, and solve for $b_0$ and $b_1$:

![](images/LS_formula2-01.PNG)

and:

![](images/b1_form-02.PNG){fig-align="center"}

Given that the formulas for $b_0$ and $b_1$ are derived using the least square approach, you may see the regression equation being referred to as the **least squares line, least squares regression line or the estimated regression equation**. However, it is important to note that in this approach, we've made no distributional assumption about the data. We assumed they're fixed and not random and follow a linear trend.

Ideally, you won't have to worry about the formulas for $b_0$ and $b_1$ and this is because, there are so many statistical software that can find the least squares for you. However, a question that often pop-up is that- why do we try to minimize the sum of *squared* errors rather than just the errors themselves? The reason is that if we didn't square the residual error above, the positive and negative residual errors would cancel each other out when we sum them, consequently yielding 0. This approach is referred to as *least squares* because it aims to *minimize* the sum of *squared* errors, thereby seeking to determine *the least squares.*

#### Example of Least Square Approach using data

For this, we would use the "car" dataset. This is an open dataset available in R software (and online). Very briefly, it shows the relationship between the speed and stopping distance of cars. You can downoad it from our google folder [here](https://drive.google.com/drive/folders/1JcI_Mxl-uiqD-bH8M8v6545kONw2ApE3?usp=sharing). load this dataset to your SPSS.

Traditionally, we would now compute $\hat{b_1}$ and $\hat{b_0}$ for the cars dataset. However, like I explained above, we can easily allow SPSS or other software to do this for us.

From the data, note that our x = speed and y = dist. From your data, we need to calculate the three sums of squares defined above. For simplicity, we will regard to the summation sign above as "S".

So, compute Sxy, Sxx, Syy

```         
Sxy = sum((x - mean(x)) * (y - mean(y)))
Sxx = sum((x - mean(x)) ^ 2)
Syy = sum((y - mean(y)) ^ 2)
c(Sxy, Sxx, Syy)
```

To do this in SPSS, follow this below procedure-

recall speed = x, dist = y.

1\. Compute the means of speed and dist:

-   Go to "Data\>Aggregate" in the top menu.

-   Drag the "speed" and "dist" to the "Aggregated variables.

-   In the "Function" box, select "mean" (if not already selected).

-   Click "OK" .

2\. Compute sum of speed and dist- i.e., Sxy

-   Go to "Transform".

-   Choose "Compute Variable."

-   In the "Compute Variable" dialog, give the new variable a name (Sxy).

-   In the numeric expression box, type SUM(speed - speed_mean) \* (dist - dist_mean).

-   Click "OK" to create the new variable.

-   Finally to create count the SUM of XY (because SPSS did not compute that automatically), click on "Data\>Aggregate". Then, drag Sxy to the "summaries of variable". Click on Function and select "Sum" (instead of Mean).

-   Click "OK" to see the sum of XY

**Follow the above procedure to do the same thing for Sxx and Syy.** Note that you don't have to calculate the speed_mean or dist_mean again. So technically, you're repeating step 4 & 5 above.

Ideally, your answer should be-

```         
 Sxy       Sxx,    Syy
5387.40  1370.00 32538.98 
```

Finally, we need to calculate the $\hat{b_0}$ and $\hat{b_1}$

$\hat{\beta_1}$ = Sxy / Sxx

$\hat{\beta_0}$ = mean(y) - $\hat{\beta_1}$ \* mean(x)

to calculate the $\hat{\beta_1}$ in SPSS, go to transform, give it a name (under the Target variable). Under the numeric expression, select the Sxy created above and divide it by Sxx. You should get 3.93.

Do the same thing for $\hat{\beta_0}$. This time, specify the dist_mean - $\hat{\beta_1}$ we just created \* speed_mean and you should get -17.58 in your data view.

$\hat{\beta_1}$ = 3.93

$\hat{\beta_0}$ = -17.98

#### **Interpretation**

**What do these values tell us about our dataset?**

Let's use the regression line formula again, $\hat{y} = \beta_0 + \beta_1 x_1$

or more formal way,

$\hat{y}_{i, dist} = \beta_0 + \beta_1 {x_1, speed}$

Remember $\hat{\beta_0}$ is the constant (intercept) and $\hat{\beta_1}$ is the slope.

From the result, the slope parameter $\hat{\beta_1}$ tells us that for every unit increase in speed of one mile per hour, the **mean** stopping distance increases by $\hat{\beta_1}$. It is important to note that we're talking about the mean estimate- If you remember, from the formula of equation of best fit line $\beta_0 + \beta_1 x_1$ represent the mean of y (here, distance) for a particular value of $x$ (speed). So, the slope ($\hat{\beta_1}$) tells us how the mean of distance is affected by speed.

Specifically, the $\hat{\beta_1}$ = 3.93 implies that for every unit increase in speed (of one mile per hour), the **estimated** *mean* stopping distance increases by 3.93ft. Also note the word "estimated" since $\hat{y}$ is the estimated mean of the observed response Y, so $\hat{\beta_1}$ tells us how the estimated mean of Y is affected by changing $x$.

**What does** $b_0$ **tell us?**

In a very brief term, the intercept (or $\hat{\beta_0}$) represent the value of Y when all the predictors = 0 (i.e.- **mean** stopping distance for a car traveling zero miles per hour or not moving at all).

Here, the $\hat{\beta_0}$ tells us that a vechicle travelling at zero mile per hour is predicted to have -17.58 stopping distance. In other words, the estimated mean stopping distance for a car not moving is−17.58 ft. Obviously this doesn't make sense, because does that mean when you apply the brakes to a car that is not moving, it moves backwards? Anyways this is not surprising becuase we "extrapolated" beyond the range of the x values (model scope). It doesn't make sense to say you're travelling at speed of zero miles per hour (so intercept here doesn't make much sense)! More information on extrapolation can be found in this [blog](https://blog.minitab.com/en/adventures-in-statistics-2/regression-analysis-how-to-interpret-the-constant-y-intercept) and we will see more of it shortly below in the tutorial.

### Predictions

Let's rewrite the estimated regression line

$\hat{y}_{i, dist} = \beta_0 + \beta_1 {x_1, speed}$

$\hat{y}_i = -17.58 + 3.93x$

or

$\hat{dist}_i = -17.58 + 3.93speed$ (n.b- recall the hat there means "predicted")

We can now use this equation to make predictions. If you check the data, the speed ranges from 4 to 25.

**Question 1**- Can we make a prediction for the stopping distance of a car traveling at 9 miles per hour?.

***Hint- It's easy, slot in 9 in the equation above (***$-17.58 + 3.93speed$***). Go to transform, compute variable and do the samething we've been doing. Note, that the speed = 9. Your answer should be 17.81 (***This tells us that the estimated mean stopping distance of a car traveling at 9 miles per hour is 17.81***).***

**In the same way, we can make predictions for unknown data or unseen time point.** For example,

**Question 2-** make a prediction for the stopping distance of a car traveling at 6 miles per hour. This is referred to as **interpolation** as 6 is not an observed value of speed (But is in the data range.)- you can check the speed and you'd agree that 6 is not there. Your answer should be **6.02.**

Finally, we can make a prediction for the stopping distance of a car that is outside of the data range (**extrapolation!**)

**Question 3-** make a prediction for the stopping distance of a car traveling at 100 miles per hour. This is extrapolation as 100 is not within the range of speed and not an observed value. So, we are only transferring our model to that time point (learning from the model- Machine Learning)! ***Your answer should be = 375.66ft***

Although cars can travel 100 miles per hour today (although with fines from police), but maybe not some years ago! This is quite similar to the similar issue when interpreting $\hat{b_0} = -17.58$ (that is estimated mean stopping distance when speed = 0). This implies that we should be less confident in the estimated linear relationship outside our data range.

### Residuals

Recall our residual formula, $e_i = y_i - \hat{y_i}$

We can calculate the residual for the prediction we made for a car traveling 9 miles per hour. First, we need to obtain the observed value of distance for this speed value (10). You can look at the data table, what's the value of distance when speed = 9?

Answer, dist == 10.

Then, we calculate our $e = 10 - 17.81$ = -7.81.

The negative residual value indicates that the observed stopping distance is actually 7.81 feet less than what was predicted.

### Estimating the Variance

We can now use the residuals for each data point to compute the variance

In regression case, for each $y_i$, we can use a different estimate of the mean, that is $\hat{y_i}$ to calculate the variancee.

To compute the variance in SPSS, you can follow these steps.

1.  compute the predicted $\hat{y} = \hat{\beta_0} + \hat{\beta_1}$
2.  Compute the residuals $e_i = y_i - \hat{y_i}$
3.  Compute the n = $length(e)$
4.  Compute the variance, $s2_e = \frac {sum(e^2)} {(n-2)}$

To do this in SPSS, go to Transform and create a variable call y_pred using the $\hat{\beta_0}$ and $\hat{\beta_1}$ already defined above. Also, do the same thing to compute residual (e). Note that your $y_i$ = dist and we've already calculated $\hat{y_i}$ up there. Lastly, n = 50 (the length of the data).

Now, compute the variance by going back to "transform" and under the numeric expression, use the SUM (e \*\*2) . Note- the e is your residual we just created and n = 50.

Finally, click on the "Data\>aggregate" and drag in the ss2_e we just calculated to the "Summaries of variables". Then, click on FUNCTION and change it to SUM. Click on OK and you should get your variance (236.53).

Similar to the univariate measure of variance, variance = 236.53 lacks a meaningful practical interpretation in this context of stopping distance. However, by taking the square root, we can obtain the standard deviation of the residuals, often referred to as the *residual standard error.*

**Question- take the square root to compute the residual standard error**

(Hint- you can do that under transform\> compute variable). Your residual standard error should be = **15.38.**

**Interpretation**- This indicates that our average estimates of stopping distance are generally inaccurate by approximately 15.38 feet.

### Variation Decomposition

Here, we will briefly define 3 of the metrics used to decomposition of variation.

#### 1. Sum of Squares Total

The term "Sum of Squares Total," denoted as SST, represent the total variation present in the observed y values.

![](images/SST.PNG){fig-align="center"}

#### 2. Sum of Squares Regression

The term "Sum of Squares Regression," often abbreviated as SSReg, denotes the portion of variation in the observed y values that can be accounted for or explained by the regression.

![](images/SSReg.PNG){fig-align="center"}

#### 3. Sum of Squares Error

The term "Sum of Squares Error," (SSE), denotes the portion of variation in the observed y values that remains unexplained or unaccounted for. You may frequently see SSE written as RSS, which stands for "Residual Sum of Squares."

![](images/SSE.PNG){fig-align="center"}

Now, you can use the formula of each of them to calculate their values in SPSS, Go to "transform\> compute variable."

-   SST = sum((dist - mean(dist)) \^ 2)

-   SSReg = sum((y_hat - mean(dist)) \^ 2)

-   SSE = sum((y - y_hat) \^ 2)

Also, calculate the aggregate (under "Data\>aggregate") to see the sum of them-

SST should be == 32538.98

SSReg = 21185.46

SSE = 11353.52

When looking at these 3 metrics individually, they kind of lack a significant practical interpretation. However, we will see now and we can use them collectively to show a new statistic that can measure the strength of regression model.

### Coefficient of Determination

The coefficient of determination is the fraction of the observed variation in y that can be accounted for or explained by the regression model.

$$R^2 = \frac {SSReg} {SST}$$

or

$$R^2 = 1- \frac {SSE} {SST}$$

**Question**- compute the $R^2$ for our example data in SPSS

Hint- Go to transform\>compute variable. Then Select the SSReg_sum/SST_sum. Your answer should be 0.65%

Interpretation- From our example dataset, our calculated $R^2$ = 0.65. Thus, we can conclude that 65% of the observed variation in stopping distance can be explained by the linear relationship with speed.

#End.

# Chapter 6- Factor Analysis

Factor analysis is a method of dimensionality (data) reduction. In research, it is common to encounter situations where we have to quantify phenomena that cannot be directly accessed (known as a latent variable) and factor analysis can be used to seek underlying unobservable (latent) variables that are reflected in the observed variables (manifest variables). Imagine you're conducting a survey to measure "burnout" and interested in determining whether the questions in the survey shows common response patterns, essentially forming a unified concept or construct. Factor analysis operates under the fundamental premise that within a group of observed variables, there exist underlying factors, which are fewer in number than the observed variables themselves, capable of explaining the connections among these variables. Note that we cannot measure burnout directly as it has many dimensions. However, it is possible to measure different aspects of burnout including motivation and stress levels. With this, it may be possible to know whether these different dimensions represent a single variable. Put simply, are these different (observable) dimensions driven by the same underlying variable?

To further illustrate, suppose you've carried out a survey to gather responses on individuals' anxiety levels related to using SPSS. Since we cannot measure SPSS anxiety directly, one may ask whether all these survey items genuinely assess what we can call "SPSS Anxiety."

For this lab, we will use the SAQ (SPSS Anxiety Questionnaire) as introduced by Andy Field (see- *Page 584 of the book Discovering Statistics Using IBM SPSS Statistics).*

For the sake of simplicity, we'll focus on the "[SAQ10](https://drive.google.com/drive/folders/1AeDF4phZO-N62LHoI4Jos1Tx-v0YCRw2?usp=sharing)," comprising the first ten items within the SAQ. You can download the data [here](https://drive.google.com/file/d/1i409ic5MvaDlu--tfaWwRNgERACGhKZh/view?usp=sharing). The SAQ10 questionnaire comprises the following questions:

1.  Statistics makes me cry

2.  My friends will think I'm stupid for not being able to cope with SPSS

3.  Standard deviations excite me

4.  I dream that Pearson is attacking me with correlation coefficients

5.  I don't understand statistics

6.  I have little experience of computers

7.  All computers hate me

8.  I have never been good at mathematics

9.  My friends are better at statistics than me

10. Computers are useful only for playing games

To begin with, let's calculate the Pearson Correlation of the data

To do this in SPSS, you can go to *Analyze\>Correlate\>Bivariate*:

![](images/Cor_plot_2.png){fig-align="center"}

From the output, it's evident that there are correlations between most of the items, spanning from a low of r = -0.38 for the pair q3 and q7 to a high of r = 0.51 for Items 6 and 7. Given the relatively high correlations among these questions, this dataset is a suitable candidate for factor analysis. Recall that the primary objective of factor analysis is to model the relationship between variables with a smaller (latent) variables. These relationships can be divided into multiple components.

### Variance Partitioning in factor analysis

Given that the goal of factor analysis is to model the "interrelationships" among variables, so instead of the mean, the focus here would be the variance and covariance. I.e, how those items/variables vary or covary.

In factor analysis, the assumption here is that variance can be partitioned into two key types: *common* and *unique* variance.

-   **Common variance** denotes the shared variance among a set of items. Items exhibiting strong correlations will have a higher degree of shared variance.

    -   **Communality** (also called $h^2$) is a measure of common variance and ranges between 0 and 1. Values closer to 1 indicate that the derived factors explain a greater portion of the variance within an individual item.

-   **Unique variance** represents any part of the variance that is not common. It can be divided into two:

    -   **Specific variance**: This variance is unique to a particular item. For instance, Item 4 "All computers hate me" might have variance linked to anxiety about computers apart from anxiety related to using SPSS.

    -   **Error variance:**  Arising from measurement inaccuracies and essentially accounting for any unexplained variance not attributed to common or specific sources.

The figure below illustrates the interconnections between these concepts:

![](images/Variance_diag.png){fig-align="center"}

Total variance comprises both common and unique variance, with unique variance further divided into specific and error variance components. When the total variance is = 1, communality is represented by ℎ², and the unique variance can be calculated as 1 - ℎ².

In the context of our SAQ10 example, SPSS Anxiety contributes to the common variance shared among all the items (that is what we want to measure anyway). However, each individual item contains both specific and error variance. Let's take Item 10, for instance, which states, "Computers are useful only for playing games." While SPSS Anxiety explains a portion of the variance in this item, there may exist systematic factors like technophobia, as well as non-systemic factors that cannot be attributed to either SPSS anxiety or technophobia, such as receiving a speeding ticket just before arriving at the survey center (**a measurement error**). Now that we have an idea of variance partioning, we can proceed to conduct our initial factor analysis. It's worth noting that the assumptions we make regarding the partitioning of variance influence the choice of analysis we employ.

## Factor Analysis in SPSS

As a researcher or analyst, our goal in factor analysis is dimensionality reduction. Put simply, to reduce the number of variables to explain and interpret the results.

We can do this using two steps: *factor extraction* and *factor rotation*

Factor extraction involves making a decision regarding both the model type and the quantity of factors to be extracted. Following factor extraction, factor rotation can be used with the aim of attaining a straightforward arrangement to enhance interpretability.

### Factor Extraction

There are two approaches to factor extraction which follows different approaches to variance partitioning: a) *principal components analysis* and b) *common factor analysis*.

### Principal Components Analysis

Unlike factor analysis, principal components analysis (PCA) assumes that there is no unique variance and the total variance is equal to common variance. Recall that we said total variance can be partitioned into *common* and *unique* variance. Thus, if there is no unique variance then common variance is equal to total variance. In other words, if the total variance is 1, then the common variance is equal to the communality = $h^2$ = 1 (see the figure above).

#### Do this in SPSS- run a PCA with 10 components

The objective of a PCA is to replicate the correlation matrix using a reduced number of components that are linear combinations of the original set of items (we shall talk a bit more on PCA during next tutorial). Although the following analysis deviates from the main PCA approach, for pedagogy reason, we will initiate the process by extracting the maximum number of components. This will help us determine the optimal number of components for extraction at a later stage.

To do this in SPSS, go to:

1.  Analyze\> Dimension Reduction \>Factor. You can move all the observed variables over the Variables: box to be analyzed.
2.  Click on Extraction, then under the Method, select Principal components and make sure to Analyze the Correlation matrix.
3.  We also request the Unrotated factor solution and the Scree plot. Under Extract, choose Fixed number of factors, and under Factor to extract enter 10.
4.  We also want to increase the Maximum Iterations of Convergence to 100.

#### Eigenvalues and Eigenvectors

Before we move on, let's do a quick introduction of eigenvalues and eigenvectors.[\
](https://stats.idre.ucla.edu/wp-content/uploads/2018/05/fig4-2a.png)

**Eigenvalues** denote the total variance that a specific principal component can account for. While theoretically they can be either positive or negative, in practice, they exclusively account variance and always positive.

-   If eigenvalues \> 0, then that's good.

-   Given that variance is inherently non-negative, negative eigenvalues signal issues with the model's stability.

-   Eigenvalues close to zero suggest the presence of multicollinearity among items, as the first component absorbs nearly all the variance.

Eigenvalues are also the summation of squared component loadings for all items within each component. This characterizes the extent to which a principal component can explain the variance in each item.

**Eigenvectors** serve as a weight for each eigenvalue. The eigenvector $\times$ the square root of the eigenvalue = the **component loading** (which can be interpreted as the correlation of each item with the principal component). In other words, eigenvectors can be calculated as the component loading divided by the square root of the eigenvalue.

$$eigenvector = \dfrac {component \hspace1ex loading} {\sqrt{ \left( eigen \hspace1ex value \right)}} $$

For this dataset, given that the eignevalue of the first item = 3.301 (check the "total variance explained" table) and the component loading for item 1 =0.637 (check the component matrix), we can then compute the eigenvector associated with the first item using the above formula

component loading = 0.637, eigenvalue = 3.301

$$eigenvector = \dfrac {0.637} {\sqrt{ \left( 3.301 \right)}} $$

eigenvector of the first item = 0.35

Note that the component loading = 0.637- Thus, we can say that the correlation of the first item with the first component is 0.637 (about 63.7%).

#### Component Matrix

The components can be seen as the correlation between each item and the respective component. Each item is associated with loadings for all 10 components. For instance, Item 1 is correlated at 0.637 with the first component, 0.182 with the second component, and -0.360 with the third, etc.

The square of each loading represents the proportion of variance explained (think of it as an $R^2$ statistic) by a particular component. For Item 1, $(0.637)^2$=0.406. This can be interpreted that 40.6% of its variance is explained by the first component. Also, $(0.182)^2$=0.033 or 3.3% of the variance in Item 1 is explained by the second component. Note that the total variance explained by both components is thus 40.6%+3.3%=43.9%. When you accumulate these squared loadings progressively through the components, the total sums up to 1 or 100%. This is referred to as the **communality**, and in a PCA the communality for each item equates to the total variance.

![](images/component_matrix_1.PNG){fig-align="center"}

N.B: If you sum the squared component loadings across the components (columns), you'd get the communality estimates for each item. Also, if you sum each squared loading down the items (rows), you'd get the **eigenvalue** for each component.

For example, if you want to manually get the first eigenvalue, take a square of each item in column 1 and add them together- you'd get = 3.301 (the eigenvalue!). And if you'd do this for other columns, you'll get ten eigenvalues for ten components- and that leads us to the next table.

#### **Total Variance Explained**

Recall that the eigenvalue = overall variance that can be explained by a given principal component. Starting with the first component, each subsequent component is derived by factoring out the influence of the previous one. Therefore the first component explains the most amount of variation, and the last component explains the least. If you look at the "Total Variance Explained" table, you'll find the total variance explained by each component. For instance, Component 1 = 3.301, or (3.301/10)%=33.01% of the total variance. Because we extracted the same number of components as there are items, the Initial Eigenvalues column should be the same as the Extraction Sums of Squared Loadings column.

![](images/total_var_1.PNG){fig-align="center"}

#### Selecting the optinal number of components to extract

Since the goal of running a PCA is to reduce our set of variables down, it would be useful to have a criterion for selecting the optimal number of components that are of course smaller than the total number of items. One criterion is the choose components that have eigenvalues greater than 1. Under the Total Variance Explained table, we see the first two components have an eigenvalue greater than 1. This can be confirmed by the Scree Plot which plots the eigenvalue (total variance explained) by the component number.

Given that the aim of conducting a PCA is to reduce our variable set, it becomes valuable to establish a criterion for selecting the optimal number of components, which should ideally be fewer than the total number of items. One such criterion involves selecting components with eigenvalues \> 1. If you look at the "Total Variance Explained" table, you'd see that the first three components possess eigenvalues \> 1. We can also further confirm this by looking at the Scree Plot, which charts the eigenvalue (total variance explained) against the component number.

![](images/scree_plot_1.PNG){fig-align="center"}

Remember that we selected the Scree plot here- dimension reduction\>factor\>extraction\>scree plot. So, the scree plot should be produced automatically.

The first component consistently accounts for the highest total variance, while the last component will always explains the least. However, the main question is: where do we see the most significant drop in variance? For this example, if you look at component 3, it shows an "elbow" joint, signifying that it might not yield substantial benefits to continue extracting more components after this. There exist varied interpretations of the scree plot, but some suggest selecting the number of components to the left of this "elbow." Following this criterion would lead us to opt for just two components. Another, more subjective perspective on the scree plot proposes that any number of components within the range of 1 to 4 could be valid, and additional supporting evidence would be beneficial.

Certain criteria propose that the total variance explained by all components should fall within the range of 70% to 80% variance, which in this scenario would indicate about five to six components. However, as noted by Andy Field (*the author of Discovering Statistics using IBM SPSS Statistics*), this approach may be implausible in social science research, where extracted factors typically explain only 50% to 60% of the variance.

Choosing the number of components involves a degree of subjectivity and ideally necessitates input from the entire research team. But for now, let's suppose we consulted Dr. Steven Shaw, and he believes that a three-component solution aligns with the study's objectives. Therefore, we can proceed with the analysis based on his input.

#### PCA with 3 components

To run the three component PCA is just as easy as running the 10 component solution we did before. The only difference is under Fixed number of factors -- Factors to extract you have to enter 3.

Follow the same steps we did before

go to:

1.  Analyze\> Dimension Reduction \>Factor. You can move all the observed variables over the Variables: box to be analyzed.
2.  Click on Extraction, then under the Method, select Principal components and make sure to Analyze the Correlation matrix.
3.  Request the Unrotated factor solution and the Scree plot. Under Extract, choose Fixed number of factors, and under Factor to extract enter **3**.
4.  Increase the Maximum Iterations of Convergence to 100.

Now, let's focus on the differences in the output between the ten and three-component solution. If you look at the Total Variance Explained, we can see that the Initial Eigenvalues no longer equals the Extraction Sums of Squared Loadings. Specifically, The main difference is that there are only three rows of Extraction Sums of Squared Loadings, and the cumulative percent variance goes up to 56.671%.

![](images/Total_var_3pc.PNG){fig-align="center"}

Also, we can see that the Component Matrix has the same loadings as the ten-component solution but instead of ten columns it's now three columns.

![](images/component_matrix_3pc.PNG){fig-align="center"}

Interpretations- Again, we will interpret Item 1 as having a correlation of 0.637 with Component 1. From the table, we can see that q07 (*All computers hate me*) has the highest correlation with Component 1 and q02 (*My friends will think I'm stupid for not being able to cope with SPSS*) has the lowest. Also, we see that q9 has the highest correlation with Component 2 and q10 the lowest. Similarly, we can see that q06 has the highest with component 3 and q09 the lowest.

#### Test:

True or False

1.  The elements of the Component Matrix are correlations of the item with each component.

2.  The sum of the squared eigenvalues is the proportion of variance under Total Variance Explained.

3.  The Component Matrix can be thought of as correlations and the Total Variance Explained table can be thought of as $R^2$.

### Communalities

Communality is calculated by summing the squares of the component loadings for the number of components you have extracted.

Look at the SPSS output you will see a table of communalities.

![](images/communalities_3pc.PNG){fig-align="center"}

Given that PCA is an iterative estimation process, it starts with 1 as an initial estimate of the communality (since this is the total variance across all 10 components), and then proceeds with the analysis until a final communality is extracted. Now, as a practice exercise, let's manually calculate the first communality from the Component Matrix. The first ordered is (0.637,0.182, -0.360) which represents the correlation of the first item with Component 1, 2, and 3.

$$h^2 = (0.637)^2 \hspace1ex + \hspace1ex (0.182)^2 \hspace1ex + \hspace1ex (-0.360)^2$$

If you do this, you should get \~= 0.569 (extraction 1 under the communality table above).

*Recall that to get the communality, you should sqaure the loadings and sum down the components*

Going back to the Communalities table, if you sum down all 10 items (rows) of the Extraction column, you get 5.667. Also, If you go back to the Total Variance Explained table and summed the first three eigenvalues you also get 3.301+1.333+1.033=5.667. Is that surprising? **Basically this implies that summing the communalities across all items is the same as summing the eigenvalues across all components**.

#### Test

1\. *In a PCA, when would the communality for the Initial column be equal to the Extraction column?*

Hint- When you run a full component PCA as the number of items. In this case, 10-component PCA.

**True or False**

1.  The eigenvalue represents the communality for each item.

2.  For a single component, the sum of squared component loadings across all items represents the eigenvalue for that component.

3.  The sum of eigenvalues for all the components is the total variance.

4.  The sum of the communalities down the components is equal to the sum of eigenvalues down the items.

**Solutions:**

1\. F, the eigenvalue = total communality across all items for a single component, 2. T, 3. T, 4. F (we sum communalities across items, and sum eigenvalues across components, which should be equal- we just did that now).

### Common Factor Analysis

The difference between principal components analysis (PCA) and common factor analysis lies in how they partition variance. Both methods are used for dimensionality reduction of the dataset to a smaller number of unobserved variables. However, PCA assumes that common variances account for all the total variance, while common factor analysis presumes that total variance can be divided into common and unique variance. The latter assumption is often more realistic, acknowledging that the measurement of the item set may not be perfect. The unobserved or latent variable responsible for common variance is termed a factor, hence the name "factor analysis."

Another significant difference between PCA and factor analysis pertains to the objective of the analysis. If the goal is simply to reduce the list of variables into a linear combination of smaller components, you can just run a *PCA*. However, if there is a prior knowledge that a latent construct underlies the interrelationships among the items, *factor analysis* may be more suitable.

In our example, we assume the existence of a construct ("SPSS Anxiety") that explains the correlations among all items in the SAQ10. However, it's recognized that SPSS Anxiety cannot account for all the shared variance among SAQ items, so we can also model the unique variance.

From the PCA result above, we can run a three-factor extraction.

#### Run a Common Factor Analysis with 3 factors in SPSS

To do this in SPSS, use the same steps as running a PCA above-Analyze\> Dimension Reduction \> Factor. Select "Extraction" and under Method choose **Principal axis factoring**. Let's keep the Maximum Iterations for Convergence at 100 and run the analysis.

Note the main difference is under the EXTRACTION METHOD- here, we see PAF (Principal Axis Factoring) instead of PC (Principal Components). We will get three tables and 1 scree plot as output, Communalities, Total Variance Explained and Factor Matrix.

Let's go over each of these and compare them to the PCA output.

Communalities of the PAF

![](images/communalities_3pc_paf.PNG){fig-align="center"}

From this table, the most obvious difference between this communalities and the one we did earlier for PCA is that the initial extraction is no longer 1. Remember that for a PCA, we assume the total variance = common variance or communality (so we pick 1 as our best initial guess). However, for principal axis factoring, instead of guessing 1 as the initial communality, it chooses the squared multiple correlation coefficient $R^2$.

Let's see this in action- at least for first item in the list  We can do this by running a linear regression where q01 is the response variable and Items 2 -10 are predictors.

#### Test

RUn a linear regression in SPSS-

Hint- *Analyze \> Regression \>Linear and enter q01 under Dependent and q02 to q10 under Independent(s)*.

You should get output similar to the one below

![](images/model_sum_q01.PNG){fig-align="center"}

Note that our $R^2$ here = 0.295 and that matches the initial communality estimate for q01 (see the communality table above). Equally, we can do the remaining 9 linear regressions in order to get all ten communality estimates but we don't have to since SPSS did that already (the communality table). Similar to PCA,  factor analysis also utilizes an iterative estimation process to obtain the final estimates under the Extraction column. Finally, if we sum all the rows of the extraction column, and we get 3.98. This represents the total common variance shared among all items in our 3-factor analysis.

### Total Variance Explained

Let's check the Total Variance Explained table. If we compare this to the table from the PCA, we can see that the Initial Eigenvalues are the same and includes 10 rows for each "factor".

![](images/Total_var_3pc_paf-01.PNG){fig-align="center"}

Indeed, SPSS leverages the information obtained from the PCA analysis for use in the factor analysis. The factors used in factor analysis are essentially the components found in the "Initial Eigenvalues" column. The primary distinction lies in the "Extraction Sums of Squared Loadings" column. Here, each corresponding entry in the Extraction column is lower than the Initial column. ***This outcome aligns with our expectations, as we assume that the total variance can be divided into common and unique variance.*** Consequently, the common variance explained will be relatively reduced. From the result, we can see that Factor 1 accounts for 27.342% of the variance, Factor 2 explains 6.889% of the variance, while Factor 3 explains 5.543% of the variance. Much like in PCA, the more factors you extract, the less variance is explained by each successive factor.

#### Test

In theory, when would the percent of variance in the Initial column ever equal the Extraction column?

(***Hint***- When there is no unique variance- recall that PCA assumes this whereas common factor analysis does not, so this is in theory and not in practice)

### Factor Matrix

If you remember, we set the number of iteration to 100. Like I said before, this is necessary to reach convergence. Look at the table, you'd see that 49 iterations were required. If we had simply used the default 25 iterations in SPSS, we would not have reached convergence or obtained an optimal solution. Hence, in practice, it's always good to increase the maximum number of iterations.

![](images/factor_matrix_paf_1-01.PNG){fig-align="center"}

From the table, the elements of the Factor Matrix table are called **loadings** and denotes the correlation of each item with the corresponding factor. Similar to PCA, if we square each loading and sum down the items (rows), we would get the total variance explained by each factor. N.B.: they are no longer called eigenvalues as in PCA but *Sums of Squared Loadings*.

Let's calculate this for Factor 1:

$$(0.575)^2 + (-0.280)^2 + (-0.602)^2 + (0.640)^2 + (0.561)^2 + (0.596)^2 + (0.662)^2 + (0.456)^2 + (-0.272)^2 + (0.405)^2$$

$== 2.73$ which matches the first row under the Extraction column of the Total Variance Explained table. We can repeat this for Factor 2 and get the same results for the second row (0.689).

Also, we can get the communality estimates by adding the squared loadings across the factors (columns) for each item. For example, for q01:

$(0.575)^2 + (0.60)^2 + (0.298)^2 = 0.423$

N.B.: these results match the value of the Communalities table for q01 (see the Extraction column). This means that the sum of squared loadings across factors $=$ communality estimates for each item. You can do the same for all other items on the list and you'd get the same result as in "Extraction" of the communality table.

### The relationship between the three tables

To understand the relationships among the three tables, let's begin with the Factor Matrix (referred to as the Component Matrix in PCA). We'll use the term "factor" to describe components, applicable to PCA as well. Within this matrix, you'll find the elements that signify the correlation between each item and each respective factor.

Now, when you square each of these elements, you obtain squared loadings, which represent the proportion of variance explained by each factor for each item. The cumulative sum of these squared loadings across factors provides the proportion of variance explained collectively by all the factors in the model. This is known as common variance or communality, and it's the outcome displayed in the Communalities table.

For the Factor Matrix, if you square the loadings and sum them down the items, you get the Sums of Squared Loadings (in PAF) or eigenvalues (in PCA) for each factor. These values now become the elements within the Total Variance Explained table.

When you sum down the rows, which is to say, you add up the values along each factor, under the Extraction column, you arrive at a total, let's say, $2.734 + 0.689 + 0.554 = 3.977$. This value represents the overall (common) variance explained by the three-factor solution for all ten items. Equivalently, because the Communalities table represents the total common variance accounted for by both factors for each item, adding up the values down the items in the Communalities table similarly yields the total (common) variance explained.

$(0.423)+(0.308)+(0.465)+(0.465)+(0.347)+(0.709)+(0.477)+(0.271) + (0.331) + (0.182)=3.978$ -\> *the same result we obtained from the Total Variance Explained table*.

To summarize:

1.  By squaring the elements within the Factor Matrix, you get the squared loadings.

2.  The summation of the squared loadings from the Factor Matrix across the factors provides the communality estimates for each item, as found in the Extraction column of the Communalities table.

3.  Summing the squared loadings within the Factor Matrix along the items yields the Sums of Squared Loadings (in PAF) or eigenvalues (in PCA) for each factor across all items.

4.  When you add up the eigenvalues or Sums of Squared Loadings in the Total Variance Explained table, you get the total common variance explained.

5.  Adding up all the items in the Communalities table is equivalent to summing the eigenvalues or Sums of Squared Loadings down all the factors, which can be found under the Extraction column of the Total Variance Explained table.

#### Test

**True or False** (the following assumes a three-factor Principal Axis Factor solution with 10 items)

1.  The entries within the Factor Matrix represent the correlations between each item and a factor.

2.  Each squared element in the Factor Matrix for Item 1 represents the communality.

3.  The summation of these squared entries in the Factor Matrix across all 10 items within Factor 1 corresponds to the first Sums of Squared Loading in the Extraction column of the Total Variance Explained table.

4.  Adding all 10 items in the Extraction column of the Communalities table provides the total common variance explained by both factors.

5.  The total common variance explained is calculated by summing all Sums of Squared Loading values in the Initial column of the Total Variance Explained table

6.  The cumulative Sums of Squared Loadings in the Extraction column under the Total Variance Explained table represents the total variance,comprising both total common variance and unique variance

7.  In common factor analysis, the sum of squared loadings is referred to as the eigenvalue.

Hints- 1, 3,4 T, 2. F, it's the sum of the squared elements across factors, 5. F, sum all eigenvalues from the Extraction column of the Total Variance Explained table, 6. F, the total Sums of Squared Loadings represents only the total *common* variance excluding unique variance, 7. F, eigenvalues are only applicable for PCA.

# Chapter 7- Bootstrapping

When we collect data, our primary interest typically lies in understanding the characteristics of the population from which we sample the data. Ideally, we use estimates derived from this sample to draw inferences about the parameters of that population. For instance, suppose we take a random sample of data for job satisfaction for employees from 3 different companies and the sample mean = 201. This value represent an estimate of the mean current salary for the entire employee population. Furthermore, suppose this estimate comes with a standard error of \$2.32 for a sample size of 65, and so a 95% confidence interval for the population's mean current salary is around \$133.45 to \$204.51. An important question here is how reliable are these estimators? In statistical analysis, when dealing with well-understood and "known" populations with stable parameters, we have ideas about the characteristics of sample estimates and can have confidence in the results. However, for unknown populations (which is quite common!) and perhaps ill-behaved parameters, we would have less confidence in the analysis. Bootstrapping is a statistical technique aimed at unveiling additional insights into the properties of estimators when dealing with "unknown" populations and parameters that may exhibit irregular behavior. In bootstrapping we *resample* the data to make statistical inference. Using this approach, we assume/pretend that the sample itself is the larger population and that allows us to approximate the effect of sampling variability by *resampling* from the *sample*. This is because, in most cases, it is practically impossible to obtain repeated samples from the same random process responsible for generating our data in order to observe how our estimate changes with each new sample. However, we can instead repeatedly draw additional samples from our original sample (population) and reapply our estimation method to each of these hypothetical samples. The variation in estimates across these resampled data can then be used to approximate the true sampling distribution of our estimator. So, this technique of assuming that our initial sample represents a theoretical population and repeatedly drawing new samples of the same size, N, from our original sample with replacement is known as **bootstrapping**.

Bootstrapping can be used to generate robust estimates of standard errors and confidence intervals for various statistics, including the mean, median, correlation, or regression coefficient. Additionally, it can be employed in constructing hypothesis tests. Bootstrapping proves most valuable as an alternative to parametric methods especially when the parametric assumptions are not met. It is also useful when parametric inference is either impractical or necessitates complex calculations to determine standard errors, such as when calculating confidence intervals for the median, quartiles, and other percentile statistics.

### 

Example 1

A telecommunications company experiences a monthly customer churn rate of approximately 27%. To effectively target efforts aimed at reducing churn, the management seeks to understand whether this percentage differs among specific customer groups they have defined. By employing bootstrapping, we can assess whether a single churn rate accurately characterizes the churn behavior across the four major customer categories.

To do this, we will use an example data that came with SPSS. The data is already in SPSS and named "telco.sav" or you can download it [here](https://drive.google.com/file/d/1tn7xiWOkFJh6hKD4p9coJkyy1yvuS3ty/view?usp=sharing).

#### Bootstrap to obtain confidence intervals for proportions

Download and load this data to your SPSS. The data contains lots of information but for now let's focus on the question- remember the question is to know whether percentage churn varies across customer groups.

Looking at the data, you'd see a column named "custcat" (customer category). First thing we need to do is to split the data by this customer category (refer back to the question!)

To do this in SPSS, from the Data Editor-

1.  select *Data \> Split File*.
2.  Then select "compare groups" and then
3.  drag the "customer category" variable into the the "Groups Based on" box.
4.  Finally, click OK to run this analysis.

Now that we have splitted the data per customer category, we can run our boostrap analysis to obtain bootstrap confidence intervals for proportions. To do this in SPSS, click on:

1.  Analyze \> Descriptive Statistics \> Frequencies.

2.  Select *Churn within last month* (churn) and drag it to the variable box

3.  Then click on Statistics.

4.  Select the Mean under the Central Tendency group.

5.  Click Continue.

6.  Now click **Bootstrap** in the Frequencies dialog box.

7.  Select Perform bootstrapping.

8.  The result may change slightly, but to ensure consistency and replicability, select Set seed for Mersenne Twister and type 9999 as the seed.

9.  Click Continue.

10. Click OK in the Frequencies dialog box.

You should get something similar to the below output

![](images/Bootstrap_1.PNG)

The statistics table displays the mean Churn within the last month for each Customer category level. Since Churn within the last month has values of 0 and 1, where 1 indicates a YES or customer who churned, the mean is = proportion of customers who churned. The Statistic column shows the values typically generated by the Frequencies analysis using the original dataset. The Bootstrap columns are results derived from the bootstrapping algorithms.

-   Bias = difference between the average value of this statistic across the bootstrap samples and the value in the Statistic column. In this scenario, the mean Churn within the last month is computed for all 1000 bootstrap samples, and the average of these means is then calculated.

-   Std. Error = standard error associated with the mean value of Churn within the last month across the 1000 bootstrap samples.

-   Also, the lower bound of the 95% bootstrap confidence interval is estimated by interpolating between the 25th and 26th mean values of Churn within the last month when the 1000 bootstrap samples are sorted in ascending order. The upper bound is similarly determined by interpolating the 975th and 976th mean values.

From this table, we can conclude that the churn rate varies among different customer types. Notably, if you look at the result, the confidence interval for Plus service customers does not overlap with any other intervals, suggesting that, on average, these customers are less likely to churn.

**Frequency Table**

![](images/Bootstrap_1_freq.PNG){fig-align="center"}

The Frequency table displays confidence intervals for the percentages (expressed as proportion × 100%) for each category. These confidence intervals are available for all categorical variables

### Example 2- Using Bootstrapping to Estimate Confidence Interval for Medians

In this example, we will use the "Employee" dataset in SPSS. The data is already in SPSS, you can navigate to your sample data or you can easily download it [here](https://drive.google.com/file/d/1fWhpc4IFfu7VULx0n9MwWlIpl_dfkjOH/view?usp=sharing).

While examining employee records, management seeks to understand the past work experience of their employees. If you check the distribution of work experience of this data, you'd see it is right skewed (to do this- click on Graphs\>Histogram). Thus, in this case, we would be less confident in using mean for estimating the "typical" previous work experience compared to the median. Unfortunately, without bootstrapping, obtaining confidence intervals for the median using SPSS becomes challenging. So now we need to do bootstrapping.

To do this in SPSS, go to-

1.  Analyze \> Descriptive Statistics \> Explore
2.  Choose Previous Experience (months) \[prevexp\] as a dependent variable.
3.  Under the "display", click "Statistics".
4.  Click Bootstrap.
5.  Select the Perform bootstrapping option.
6.  Set the seed for Mersenne Twister and type 9999 as the seed.
7.  Select Bias corrected accelerated (BCa)- this way, you can get a more accurate intervals (albeit more processing time).
8.  Click Continue.
9.  Click OK in the Explore dialog box.

You should get a result that looks like below

![](images/Bootstrap_median.PNG){fig-align="center"}

The result table above contains various statistics and bootstrap confidence intervals for these statistics. The spread observed in these bootstrap estimates provides an approximate measure of how large the impact of random error is from the original sample on the estimate variation. The bootstrap confidence interval for the mean (ranges from 86.75 to 104.74) and overlap the parametric confidence interval (86.42 to 105.30). Since both confidence intervals overlap, it means there is no statistically significant difference between the two estimation methods. In terms of the work experience, we can conclude that the "typical" employee has around 7-9 years of prior experience. However, the distribution of Previous Experience (months) is skewed, making the mean less suitable indicator of the "typical" current salary in comparison to the median.

Meanwhile, when we look at the bootstrap confidence interval for the median, it ranges from 49.50 to 61.00. This estimate is both narrower and of lower value than the mean's confidence interval and suggest that the "typical" employee has approximately 4-5 years of prior experience. By using bootstrapping, we were able to obtain a range of values that more accurately represents the typical previous experience.

### Example 3- *Bootstrapping to Choose Better (informative) Predictors*

For this task, we will use the same "Employee data" as Example 2. Here, we are interested in determining the factors related to salary increases among employees. To address this question, we will fit a linear model to the differences between current and initial salaries. When employing bootstrapping for a linear model, you can employ specialized resampling techniques, such as the residual and wild bootstrap, to enhance the accuracy of your results.

To do this in SPSS, we need to first compute the difference between Beginning and Current salary.

1.  Cllick on Transform and then Compute Variable.
2.  Under the "Target variable", type "diff_salary"
3.  Now, type *salary-salbegin* as the numeric expression.
4.  Click OK.

Once you're done calculating the difference, you should see it as a new column in your SPSS data editor. Now, we can proceed with our analysis.

1.  Click on Analyze\>General Linear Model\>Univariate

2.  Move the dif_salary to the dependent variable.

3.  Move Gender \[gender\], Employment Category \[jobcat\], and Minority Classification \[minority\]

    to the fixed factors box.

4.  Select Months since Hire \[jobtime\] and Previous Experience (months) \[prevexp\] as covariates.

5.  Now, click Model.

6.  Click Build terms and select Main effects from the Build Terms dropdown.

7.  Select gender through prevexp as model terms.

8.  Click Continue.

9.  Click Save in the GLM Univariate dialog box.

10. Select Unstandardized in the Residuals group.

11. Click Continue.

12. Click Bootstrap in the GLM Univariate dialog box.

    The settings for bootstrapping remain consistent throughout dialogs that offer bootstrapping options. During the active bootstrapping process, you are unable to save new variables to the dataset, so it is crucial to ensure that bootstrapping is turned off.

13. If necessary, uncheck the "Perform bootstrapping" option.

14. Click the "OK" button in the GLM Univariate dialog box. As a result, the dataset will now include a new variable "RES_1," which = unstandardized residuals derived from this model.

15. Again, go to the GLM Univariate dialog (Analyze\>GLM\>Univariate) and select the "Save" option.

16. Uncheck the Unstandardized residuals, then click Continue and click Options in the GLM Univariate dialog box.

17. Under the Display group, select Parameter estimates.

18. Click Continue.

19. Click Bootstrap in the GLM Univariate dialog box.

20. Click Perform bootstrapping.

21. Set seed for Mersenne Twister and type 9999 as the seed.

22. Click Continue, then finally click on OK to run the analysis

Once you click on OK, your result should look like below

![](images/param_est.PNG){fig-align="center"}

The Parameter Estimates table presents the conventional, non-bootstrapped parameter estimates for the model's terms. Since the significance level of 0.105 associated with the term \[minority=0\] \> 0.05 (our chosen alpha), we have evidence NOT to reject the null hypothesis. We can then conclude that there is no significant impact of Minority Classification on salary increases.

*Now, when we look at the table showing Bootstrap for Parameter Estimates.*

![](images/Bootstrap_param_est.PNG){fig-align="center"}

In the "Std. Error" column, you'll notice that, in some cases, the parametric standard errors, such as those for the intercept, appear to be small when compared to the bootstrap estimates, leading to wider confidence intervals. On the other hand, for certain coefficients, like \[minority=0\], the parametric standard errors were too large. However, the bootstrap results reveal a significance value of 0.037 which is \< 0.05. With this significant result, we can therefore conclude that the observed difference in salary increases between employees who are minorities and those who are not is not merely as a result of chance. As such, the company now have evidence to recognize that the difference worth further investigations and try to uncover potential underlying causes.

#End.

# Chapter 8- Revisiting Repeated Measure ANOVA

In lab 2 of this class, we discussed repeated measure ANOVA. Please see this [link](https://drhammed.github.io/EDUC806_CU/#repeated-measures-anova) before you proceed if you need a quick tutorial about one-factor repeated measure ANOVA.

A two-way repeated measures ANOVA is used to assess the mean differences among groups categorized based on two independent variables (within-subjects factors). This type of ANOVA is commonly applied in scenarios where a dependent variable is measured across two or more time points or when subjects experience two or more conditions, with "time" and "conditions" serving as the two factors. The primary goal of a two-way repeated measures ANOVA is to examine whether there is an interaction effect between these two factors on the response (dependent) variable.

For example, suppose an online retailer aim to enhance the productivity of packers in their order fulfillment center. So, the retailer wants to investigate whether having background music will impact productivity. To do this, they recruited 200 packers to participate in an experiment with a "control" group having no music and a "treatment" group with music. The retailer is also interested in understanding if any productivity increase is influenced by the timing of music playback, exploring whether it is a sustained improvement or just an initial novelty effect.

The dependent variable in this study is "productivity" measured by the average number of packages fulfilled. The two factors are "conditions" (with two groups: "control" or "treatment") and "time" (productivity at three time points: "at the beginning of the experiment," "1 week later," and "4 weeks later").

All 200 employees experience both the treatment and control conditions, but the order varies. The employees are randomly divided into two groups: (a) 100 packers start with the control and then undergo the treatment, while (b) the other 100 start with the treatment and then undergo the control (they did this for ***counterbalancing***, to mitigate bias that may arise from the order of the condition).

Upon completion of the experiment, the retailer employs a two-way repeated measures ANOVA to assess whether changes in productivity result from the interaction between the use of music (representing "conditions," one of the factors) and "time" (the second factor). Although regardless of the presence of interaction, follow-up tests can be conducted to delve deeper into such interaction.

Now suppose we have confirmed the presence of a statistically significant interaction, there are various approaches to further investigate the results (some of which we have discussed in the past. Please see the Post-Hoc part of this [tutorial](https://drhammed.github.io/EDUC806_CU/#one-way-anova)). It's crucial to recognize that the two-way repeated measures ANOVA is an [omnibus](https://www.statisticshowto.com/omnibus-test/) test statistic and doesn't provide information on specific groups within each factor that differ significantly from each other. For instance, if one factor (e.g., "time") comprises three groups (e.g., "time 1," "time 2," and "time 3"), the two-way repeated measures ANOVA outcome won't indicate whether the values on the dependent variable differ between specific pairs of groups (e.g., "Time 1" vs. "Time 2"). It will only show that at least two of the groups exhibit differences. Given the potential for multiple groups and factors in your study design, determining which specific groups differ is essential. This can be accomplished through [post-hoc](https://drhammed.github.io/EDUC806_CU/#one-way-anova) tests.

However, If you're uncertain about the suitability of a two-way repeated measures ANOVA for your study, it's worthwhile to compare it with a one-way repeated measures ANOVA and even a mixed ANOVA. The two-way repeated measures ANOVA extends beyond the one-way version, which as the name implies, involves only a single factor (i.e., one independent variable). It's also essential to differentiate between the two-way repeated measures ANOVA and the mixed ANOVA. The mixed ANOVA closely resembles the two-way repeated measures ANOVA since both involve two factors (often "time" and a form of "condition") and aim to understand whether an interaction exists between these factors and the dependent variable. However, a key distinction lies in the fact that, in a mixed ANOVA, subjects experiencing each condition (e.g., control and treatment) are different, whereas in a two-way repeated measures ANOVA, subjects undergo both conditions (e.g., both control and treatment).

Finally, if you find that the two-way repeated measures ANOVA may not be the most suitable test for your needs, considering a one-way repeated measures ANOVA (if you have one factor) or mixed ANOVA may be advisable. You can also refer back to this [table](https://drhammed.github.io/EDUC806_CU/#which-test-should-i-use-for-my-data) to select the appropriate test for your data.

### Assumptions

Like we discussed under the [one-way ANOVA](https://drhammed.github.io/EDUC806_CU/#one-way-anova), you need to make sure your assumptions are met (this is a parametric test). Such assumptions include

-   Your independent variables ( i.e.within-subjects factors) should consist of at least two categorical, "related groups" or "matched pairs".

-   **Normal distribution**- The distribution of the response (dependent) variable within each combination of related groups should show an approximate normal distribution.

-   **Dependent variable** should ideally be **continuous** (i.e., **interval** or **ratio** variables)

-   No significant outliers

-   **Sphericity**- equality of variance in the differences between all combinations of related groups.

### Practice in SPSS

Context-

For this lab, we shall use a fictional dataset "lab_9-Data." You can download it [here](https://drive.google.com/file/d/1OEbvqwu6Af9tGl-TVyLV_Qd8ytFgjNKv/view?usp=sharing). This dataset was generated randomly in Excel and consists of 22 observations. In this study, we give our participants an assessment to analyze their relaxation level (higher score indicates a higher level of relaxation). Here, we propose that light and sound level will influence the relaxation score. Each participant in this experiment is exposed to music (quiet vs loud) while simultaneously exposing them to light (light or dark). In short, each participants were exposed to all combination of the 4 conditions (every combination of the 2 factors- **light** and **music**). Both factors contains 2 levels- **Light** (light vs dark) and **Music** (quiet vs loud). Following this, relaxation scores were measured and participants were examined multiple times (repeated).

So, let us perform a Two-way Repeated Measures ANOVA in SPSS to assess if any of these interactions are significant. Specifically, analyze interactions between light and music, or the combined impact of music, and light on relaxation levels.

IN SPSS,

1.  Click [**A**]{.underline}**nalyze \> [G]{.underline}eneral Linear Model \> [R]{.underline}epeated Measures**
2.  The next pop-up should ask you to define the "Within-subject" factor. Change the name to "Light" and type 2 under the "number of levels (light vs dark)
3.  Repeat step 2 and change the name to Music and number of level = 2 (loud vs quiet).
4.  Click on Define
5.  Now, match the data to the "Within-Subject variables". Drag the "loud_light' to the "1,1", Quiet_light to 1,2, loud_dark = 2,1, and quiet_dark = 2,2.
6.  Click on "Plots" and move the "light" to the "horizontal" and Music to the "separate lines".
7.  Click on "Add" and then "continue".
8.  Click on "options" and check "Descriptive statistics" and "Estimate of effect size"
9.  Finally, click on "OK" to run the analysis.

### Results

The results produced by SPSS Statistics are comprehensive, offering quite a lot of information regarding your analysis. Here, I provide a brief overview of the key steps you should take to interpret the results of your two-way repeated measures ANOVA and, if necessary, when to conduct further analysis within SPSS Statistics.

![](images/descript stat.PNG){fig-align="center"}

Looking at the table, we can see all the combinations appear to have similar means and the only one that stands out is the room that is both loud and with light (-0.7127).

Looking at the "test of Sphericity" (which explain if the variances of the differences between all combinations of related groups are equal), you can see there's no result (sig. column is empty). That is because we have just two levels within each subject factor (no variation). So, we should not expect result for the test of sphericity in this case. However, if you have different levels in your data, then you'd get result there and can interpret the significance column with respect to the variance (again, this assumption test for equality of variance and if it's significant, the assumption is violated). You can read more about this test on this [Wikipedia page](https://en.wikipedia.org/wiki/Mauchly%27s_sphericity_test).

![](images/sphericity.PNG){fig-align="center"}

Now, Let's interpret the "Test of within-Subjects Effects".

![](images/within-subject-effects.PNG){fig-align="center"}

If you look at the main effect of "light", we can see there's no significant effect (sig. value = 0.443). Also, if you look at the main effect of "music", there's no significant effect of music (sig. = 0.240). Looking at the interaction effect (light\*music), we also have evidence to conclude that there's no significant effect since the sig. value is \> 0.05 (0.273).

In essence, if there is no **statistically significant interaction**, you should interpret and report the main effects found within the **Tests of Within-Subjects Effects** output tables. In this case, rather than calculating simple main effects, which is done when the interaction is statistically significant, the focus shifts to interpreting the main effects for your factors (i.e., the independent variables). Additionally, if there's a significant interaction, we would have to run a pairwise comparison test or a paired sample t-test. Also, if either of these main effects is statistically significant, you will need to interpret the relevant SPSS Statistics output from your **post hoc tests** in the Pairwise Comparisons table, as discussed in [Week 2](https://drhammed.github.io/EDUC806_CU/#repeated-measures-anova). This will help you to understand where the differences (between groups) within your factors lie.

Moving to the profile plot

![](images/plot_1-01.PNG){fig-align="center"}

The blue denotes the loud music and the purple = quiet condition. So, looking at the Loud music from the plot, we can observe that as we move from "light" to "dark", the score increased dramatically, and we can say the relaxation levels were much higher when the music is loud (i.e., people enjoy loud music and in a somewhat cool environment). However, looking at the quiet music condition, we can see that as we move from the light to dark area, the score increased slightly but not in the same rate as loud music.
