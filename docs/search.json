[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDUC_806- Quantitative Research Methods",
    "section": "",
    "text": "During Tutorial 2 & 3 (“Model Terminology”), I explained that Linear model and Linear regression are just synonyms and we often use either terms when quantifying the effect of a “continuous” independent variable on a”continuous” dependent (response) variable. The difference between this and ANOVA is that ANOVA is usually used when quantifying the effect of a “discrete (or categorical)” independent variable on a”continuous” dependent variable. So, it is important to note that- ANOVA is also a linear regression! In fact, if you run “anova” function on linear model object, you’ll most likely get the same p-value.\nRegression generally refers to the fact that we are quantifying the relationship between a response variable and (one or more) predictor variables. In the case of SLR, both the response and the predictor are numeric variables and we are using a single predictor (independent) variable. Later we will use multiple predictor variables (multiple regression). Also, this models tells us that our model for Y is a linear combination of the predictors X. (In this case just one predictor)! For now, this always results in a model that is a line, but this is not always the case (and we may see this later on in the semester).\nLike ANOVA, in SLR, we often talk about the assumptions that this model makes. This include-\n\nLinearity- the relationship between Y and x is linear, of the form \\(\\beta_0 + \\beta_1x\\).\nIndependent. The errors \\(\\epsilon\\) are independent.\nNormality. The errors, \\(\\epsilon\\) are normally distributed. I.e. the “error” around the line follows a normal distribution.\nEquality of Variance. At each value of x, the variance of Y is the same.\n\nFor this lab, we will be using a year dataset on Corvettes sales in Virginia Beach, Virginia. Using this data, ten Corvettes (between 1 and 6yrs old) were randomly selected and the below data shows the sales price (in hundreds of dollars) denoted by y and the age (in years) denoted by x.\n\n\n\n\n\n\n\n\nGraph the data in a scatterplot to determine if there is a possible linear relationship.\nCompute and interpret the linear correlation coefficient, r.\nDetermine the regression equation for the data.\nGraph the regression equation and the data points.\nIdentify outliers and potential influential observations.\nCompute and interpret the coefficient of determination, r2.\nObtain the residuals and create a residual plot. Decide whether it is reasonable to consider that the assumptions for regression analysis are met by the variables in questions.\nAt the 5% significance level, do the data provide sufficient evidence to conclude that the slope of the population regression line is not 0 and, hence, that age is useful as a predictor of sales price for Corvettes?\nObtain and interpret a 95% confidence interval for the slope, β, of the population regression line that relates age to sales price for Corvettes.\nObtain a point estimate for the mean sales price of all 4-year-old Corvettes.\nDetermine a 95% confidence interval for the mean sales price of all 4-year-old Corvettes.\nFind the predicted sales price of Jack Smith’s 4-year-old Corvette.\nDetermine a 95% prediction interval for the sales price of Jack Smith’s 4-year-old Corvette.\n\nThis is the link to the main google drive folder for all the data to complete Lab_5!\nTo begin, open your SPSS and load in the Corvettes_data. I’ve reprocessed it and you can download it here. if for anything that doesn’t work, you can manually enter the two variables in the SPSS.\n\n\n\n\n\n1. To graph the data in a scatterplot and determine if there is a possible linear relationship.\nSelect Graphs &gt; Scatter/Dot &gt; select Simple, then click the Define button. The Y-axis should be the Price and the X Axis variable = Age.  Click on “Titles” and you can enter a descriptive title for your graph, and click “Continue.” Click “OK”.\nYour graph should look similar to the figure below\n\n\n\n\n\nInterpretation- The points seem to follow a linear pattern, although with a negative slope.\n\nTo compute the linear correlation coefficient,\nClick on “Analyze” &gt; “Correlate” &gt; “Bivariate”. Select “Age” and “Price” as the variables and select “Pearson” as the correlation coefficient. Finally, you can run the analysis.\n\nThe Pearson Correlation is a statistical method that calculates the strength and direction of linear relationships between continuous variables. It produces a sample correlation coefficient, r, which can be used to evaluate whether there is a linear relationship among the same variables in the population. This population correlation coefficient is represented by ρ (“rho”) and is a parametric measure.\nPearson correlation indicates:\n\nWhether there is a statistically significant linear relationship between two continuous variables\nIt also shows the strength of this linear relationship (i.e., how close the relationship is to being a perfectly straight line)\nFinally, it reveals the direction of this linear relationship (increasing or decreasing)\n\nIt is however important to note that Pearson Correlation cannot address non-linear relationships or relationships among categorical variables. To address relationships that involve categorical variables and/or non-linear relationships, you need to consider the equivalent non-parametric test (e.g., Spearman’s rank correlation).\nAlso, while Pearson Correlation reveals associations among (continuous) variables, you should remember that “Correlation does not imply causation,” no matter how large the correlation coefficient is.\nInterpretation- The correlation coefficient is -0.968. This r-value indicates a robust negative linear correlation, given its proximity to -1 and negative sign. This strong negative linear correlation suggests that data points should closely cluster around a downward-sloping regression line, (which aligns with the graph above). Consequently, the presence of a strong negative linear relationship between Age and Price supports the continuation of linear regression analysis.\nRegression equation for the data.\nBecause our goal here is to predict the price of 4-year-old Corvettes, let’s modify the data variable before we proceed. Under the “Age” variable, enter the number “4” after the last row. Also, at the last row of the “Price” variable, enter a dot “.” This way, we’re telling SPSS that we want a prediction for this value and not to include the value in any other computations.\n\nSelect Analyze &gt; Regression &gt; Linear. Select “Price” as the dependent variable and “Age” as the independent variable. Click “Statistics,” select “Estimates” and “Confidence Intervals” for the regression coefficients, select “Model fit” to obtain r2, and click “Continue.”\nClick “Plots,” select “Normal Probability Plot” of the residuals and click “Continue.” Click “Save,” select “Unstandardized” predicted values, select “Unstandardized” and “Studentized” residuals, select “Mean” (to obtain a confidence interval output in the Data Window) and “Individual” (to obtain a prediction interval output in the Data Window) at the 95% level, and click “Continue.”  Finally, click “OK” to run the analysis.\n\nFrom above, the regression equation is: Price = 29160.194 – (2790.291)(Age). So what if a newly sold Corvettes was 10years old? What would be the price? Y = 29160.194 – (2790.291)(10) which equals ~ $1257.284.\nGraph the regression equation and the data points\nAnother way to visualize our regression line is through a scatterplot. Click on Graphs, then Scatter/Dot. In the Scatter/Dot dialog box that appears, be sure “Simple Scatter” is selected and then click Define. Move “Price” to the “Y Axis” box and move “Age” to the “X Axis” box and then click OK. A simple scatterplot should appear in your Output Viewer window. Double click the graph to show the “Chart Editor” window. Now, click Elements and then click Fit Line at Total. Now your scatterplot displays the linear regression line computed above while also providing you with the regression equation and the R squared value. This is a good way to double-check that we’ve written our equation correctly.\n\nIdentify the Outliers and potential influential observations\nFrom the plot, there seem to be no points that lie far from the cluster of data points or far from the regression line; thus, no possible outliers.\nCompute and interpret the coefficient of determination, r2.\n\nThe r2 = 0.937; therefore, about 93.7% of the variation in the price data is explained by age. I.e., as the car gets older, the value/price drops!  The regression equation appears to be very useful for making predictions since the value of r2 is close to 1.\nNote- we also have adjusted R-square. R-square technically measures the variation of a regression model (variation in Y given x). R-squared either increases or remains the same when new predictors are added to the model. Adjusted R-squared measures the variation for a multiple regression model, and helps you determine goodness of fit. For the purpose of this SLR (one predictor, we are not adding more), so we can decide to intepret any of them. But if you have multiple predictors, you may want to look into Adj. R2)!\nResiduals\nThe residuals, predicted values and confidence intervals can be found in the data window.\n\nTo create a residual plot, select Graphs&gt; Scatter/Dot&gt;Simple. Select  the residuals (RES_1) as the Y Axis variable and Age as the X Axis variable. Click “Titles” to enter “Residual Plot” as the title for your graph, and click “Continue”. Click “OK”.\nDouble-click the resulting graph in the output window, select “Options” &gt; “Y Axis Reference Line”, select the “Reference Line” tab in the properties window, add position of line “0”, and click “Apply”. Click the close box to exit the chart editor.\n\nDo the same scatter plot to create a studentized residual plot. Select the SRES as the Y axis in this case (Age remains the X axis). Set the title and enter “Studentized Residual Plot”.\n\nAlso, to assess the normality of the residuals, consult the P-P Plot from the regression output.\n\nInterpretation- The residual plot shows a random scatter of the points (independence) with a constant spread (variance). Also, the studentized residual plot shows a random scatter of the points (independence) with a constant variance and with no values beyond the ±2 standard deviation reference lines (no outliers). The normal probability plot of the residuals shows the points close to a diagonal line; therefore, the residuals appear to be approximately normally distributed. Thus, we have evidence to conclude that the assumptions for regression analysis appear to be met.\nQuestion - Using 5% significance level, do the data provide sufficient evidence to conclude that the\nslope of the population regression line is not 0 and, hence, that age is useful as a predictor\nof sales price for Corvettes?\nHere, we need to state the hypothesis. The hypothesis from the question above is that-\n\\(H_0\\): \\(\\beta = 0\\) (Age is not a useful predictor of price.)\n\\(H_a\\): \\(\\beta \\neq 0\\) (Age is a useful predictor of price.)\nNow that we’ve done that, remember our alpha = 0.05 and our critical value or rejection region is that we should reject the null hypothesis if alpha is less than 0.05. Finally, to answer the question, you can choose your test Statistic (choose either the T-test method or the F-test method- not both!). Check the regression output and see the “coefficient”. From the result of the regression, you can see that T = –10.887, and p-value = &lt;0.001\n\nInterpretation- Since the P-value &lt; 0.05, we have evidence to reject the null hypothesis. In other words, we have enough evidence to conclude that the slope of the population regression line is not zero. In other words, age is a useful predictor of price for Corvettes.\nObtain and interpret a 95% confidence interval for the slope, β , of the population regression line that relates age to sales price for Corvettes.\nLook at the regression coefficient above, We are 95% confident that the slope of the true regression line is somewhere between –3381.295 and –2199.288. In other words, we are 95% confident that for every year older Corvettes get, their average price decreases somewhere between $3,381.295 and $2,199.288.\nObtain a point estimate for the mean sales price of all 4-year-old Corvettes.\nLook at the data window (last row), the point estimate (PRE_1) is 17999.02913 dollars ($17,999.02913).\nDetermine a 95% confidence interval for the mean sales price of all 4-year-old Corvettes.\nStill on the last row of the data table, we are 95% confident that the mean sales price of all four-year-old Corvettes is somewhere between $16,958.46042 (LMCI_1) and $19,039.59783 (UMCI_1).\nFind the predicted sales price of Jack Smith’s selected 4-year-old Corvette.\nThe predicted sales price is 17999.02913 dollars ($17,999.02913).\nDetermine a 95% prediction interval for the sales price of Jack Smith’s 4-year-old\nCorvette.\nWe are 95% certain that the individual sales price of Jack Smithʼs Corvette will be\nsomewhere between $14,552.91726 (LICI_1) and $21,445.14099 (UICI_1).\n\n\n\n\n\nSo far, we have learned how to calculate a linear regression equation and make predictions. However, what if there are other potential independent variables to consider? In fact, It is uncommon for a dataset or research study to have only one predictor. Similarly, it is rare for a response variable to depend solely on a single variable. In this chapter, we will expand our simple linear regression (SLR) model to include multiple predictors. Multiple regression allows us to incorporate multiple independent variables and assign weights to each of them, resulting in more accurate predictions. The process of conducting a multiple regression in SPSS is similar to that of linear regression, with the difference being the inclusion of additional independent variables.\nFor this lab, we shall be using the “autompg” dataset containing car information. This dataset provides data on fuel economy from 1999 and 2008 for about 38 popular models of cars. It contains a response variable known as “mpg,” which records the city fuel efficiency of cars, alongside several predictor variables detailing the vehicle attributes. You can download the data from our google drive folder. See the link here. For anyone using R, you can find the dataset loaded with the ggplot2 package.\nBrief description of the variables in the data\n\n\n\nVariable\nType\nDescription\n\n\n\n\nmpg\nnumeric\ncity fuel efficiency\n\n\ncyl\ninteger\nnumber of cylinders\n\n\ndispl\nnumeric\nengine displacement in liters\n\n\nhp\nnumeric\nhorse power\n\n\nwt\nnumeric\nWeight\n\n\nacc\nnumeric\nacceleration\n\n\nyear\ninteger\nyear of manufacturing\n\n\n\nAt this point, our focus will be on utilizing two variables, “wt” and “year,” as predictor variables. In other words, we aim to create a model that predicts a car’s fuel efficiency (mpg) based on its weight (wt) and the model year (year). To achieve this, we will formulate the following linear model:\n\\[ Y_i = \\beta_i + \\beta_1X_1 + \\beta_2X_2 + \\epsilon_i, \\hspace4ex i = 1, 2, …,n\\]\nwhere \\(\\epsilon_i \\sim N(0,\\alpha^2)\\) , \\(x_{i1}\\) = the weight (wt) of the \\(i\\) car, and \\(x_{i2}\\) as the model year (year) of the \\(i\\) car.\nTask- before we do for multiple predictors, let’s quickly revist SLR and use one predictor. For this part, perform a simple linear regression of mpg against wt. What’s the R-squared? is weight a good predictor of mpg? (refer back to the tutorial on SLR- we just covered all of that)!\nYour model summary should be like below.\n\n\n\n\n\nOkay, now that we have done that- we can proceed to multiple linear regression. Recall, we want to build a model with mpg as dependent variable, while wt and year as independent variables.\nTo do this SPSS,\n\nClick on Analyze, then Regression, then Linear from the submenu to open the Linear Regression dialog.\nIn the Linear Regression dialog:\n\nMove the dependent variable “mpg” to the “Dependent” box.\nMove the independent variables “wt” and “year” to the “Independent(s)” box.\n\nClick the “Statistics” button to access the statistics options and check “estimates” (under regression coefficient) and model fit to request the coefficients of the linear model and model fit.\nClick “Continue” to return to the Linear Regression dialog.\nFinally, click “OK” to run the linear regression analysis.\n\nQuestions\n1) Looking at the Output Viewer window, you should sea tables that are similar to the ones in our previous example (our model summary is new). Take a look at the “model summary” to determine the new R squared and Standard Error of the Estimate. Compare that with the previous regression output- has the inclusion of additional variables (year) resulted in an improvement in our model?\n\n\n\n\n\nNote- the new R squared has improved, and the Standard Error has reduced! This indicates that our multiple regression model is more precise than the previous linear regression model.\n2) Do all our variables hold statistical significance?\nYou can verify this by checking the ANOVA table. The F statistic obtained and its significance indicate whether our independent variables have a statistically significant association with our dependent variable. It also helps us determine whether our model is a good fit for explaining the variation in the mpg.\nIn our case, we have evidence to believe it is, given that the significance (sig) is way below 0.05 (&lt;0.001).\nLastly, check the “Coefficients” table. Here we find the value of a (or the slope) for each of our independent variables (wt and year) and we also find our intercept.\n\n\n\n\n\nSo, how can we write our multiple regression equation? mpg (Y) = -14.638 – 0.07(weight) + 0.761(year). Thus, if we added a new car and had some basic data like the year and weight, we would be able to estimate, with a relatively high degree of confidence, how many mpg would be used given the predictors.\nInterpretation\nHere, the constant = -14.638 represents our estimate for the intercept, i.e.- the mean miles per gallon for a car that has a weight of 0 pounds and was manufactured in 1900 (the start year of our dataset). As we can see here that the estimate is negative, which, in the real world, is physically impossible. However, this is not surprising because we cannot realistically expect our model to accurately predict the fuel efficiency of cars from 1900 that weigh 0 pounds because such vehicles never existed anyways! So, like simple linear regression, this value (intercept) represent the mean of Y when all predictors are set to 0.\nHowever, the interpretation of the coefficients of our predictors is slightly different from previous SLR. For instance, the estimate of -0.007 for “wt” = the expected average change in miles per gallon for a one-unit increase in weight for cars of a specific model year, with the year being held constant. Note that this estimate is negative, which aligns with our expectations, as, in general, fuel efficiency tends to decrease for larger vehicles. However, in the context of multiple linear regression, this interpretation is contingent upon a fixed value for another predictor, such as “year” in our case. This means that the relationship between fuel efficiency and weight might not hold true when additional factors, like the model year, are taken into account, potentially causing a reversal in the sign of our coefficient.\nLastly, the estimate of 0.761 for “year” = the expected average change in miles per gallon for a one-year increase in the model year for cars with a specific weight, where weight is held constant now. It is not far from expectation that this estimate is positive since one would anticipate that, over time, as technology advances, cars with the specific weight would achieve better fuel efficiency compared to their earlier counterparts.\nNote- Sometimes, you may discover that the model is not statistically significant, or that one independent variable does not hold statistical significance. In such instances, you may want to rerun the model, eliminating insignificant or redundant variables. Ideally, it is good to do some variable importance selection on your predictors before including them in the model (or use some prior knowledge of the system). It may take several attempts to run multiple regression models to find the best-fitting model for the data. Generally, it is good to have model with a low standard error of the estimate, high R squared and relatively simple. A model with three independent variables, a relatively high R squared and low standard error may be preferable to a model with 19 independent variables and a high R squared and low standard error (this is why variable importance is crucial)!\nTo do variable importance selection, you can calculate the multicollinearity. In SPSS, you can compute the Variance Inflation Factor (VIF). This can be done under regression&gt; statistics&gt; collinearity. The output should be like below.\n\n\n\n\n\nThe VIF values indicate the degree of multicollinearity for each variable in the model. In our case, the VIF (for both wt and year) is around 1.104. Ideally, a VIF of 1 means that variables are not correlated and no multicollinearity in the regression model. Generally, a VIF &gt;6 is considered a sign of high multicollinearity between the predictor variables and can affect the stability and interpretability of your regression model. You may need to address multicollinearity by either removing one of the correlated variables (redundant) or you can use dimensionality reduction techniques like Principal Component Analysis (PCA) to reduce dimensions (we shall come back to PCA soon).\n#End"
  },
  {
    "objectID": "index.html#factor-analysis-in-spss",
    "href": "index.html#factor-analysis-in-spss",
    "title": "EDUC_806- Quantitative Research Methods",
    "section": "Factor Analysis in SPSS",
    "text": "Factor Analysis in SPSS\nAs a researcher or analyst, our goal in factor analysis is dimensionality reduction. Put simply, to reduce the number of variables to explain and interpret the results.\nWe can do this using two steps: factor extraction and factor rotation\nFactor extraction involves making a decision regarding both the model type and the quantity of factors to be extracted. Following factor extraction, factor rotation can be used with the aim of attaining a straightforward arrangement to enhance interpretability.\n\nFactor Extraction\nThere are two approaches to factor extraction which follows different approaches to variance partitioning: a) principal components analysis and b) common factor analysis.\n\n\nPrincipal Components Analysis\nUnlike factor analysis, principal components analysis (PCA) assumes that there is no unique variance and the total variance is equal to common variance. Recall that we said total variance can be partitioned into common and unique variance. Thus, if there is no unique variance then common variance is equal to total variance. In other words, if the total variance is 1, then the common variance is equal to the communality = \\(h^2\\) = 1 (see the figure above).\n\nDo this in SPSS- run a PCA with 10 components\nThe objective of a PCA is to replicate the correlation matrix using a reduced number of components that are linear combinations of the original set of items (we shall talk a bit more on PCA during next tutorial). Although the following analysis deviates from the main PCA approach, for pedagogy reason, we will initiate the process by extracting the maximum number of components. This will help us determine the optimal number of components for extraction at a later stage.\nTo do this in SPSS, go to:\n\nAnalyze&gt; Dimension Reduction &gt;Factor. You can move all the observed variables over the Variables: box to be analyzed.\nClick on Extraction, then under the Method, select Principal components and make sure to Analyze the Correlation matrix.\nWe also request the Unrotated factor solution and the Scree plot. Under Extract, choose Fixed number of factors, and under Factor to extract enter 10.\nWe also want to increase the Maximum Iterations of Convergence to 100.\n\n\n\nEigenvalues and Eigenvectors\nBefore we move on, let's do a quick introduction of eigenvalues and eigenvectors.\n\nEigenvalues denote the total variance that a specific principal component can account for. While theoretically they can be either positive or negative, in practice, they exclusively account variance and always positive.\n\nIf eigenvalues &gt; 0, then that’s good.\nGiven that variance is inherently non-negative, negative eigenvalues signal issues with the model’s stability.\nEigenvalues close to zero suggest the presence of multicollinearity among items, as the first component absorbs nearly all the variance.\n\nEigenvalues are also the summation of squared component loadings for all items within each component. This characterizes the extent to which a principal component can explain the variance in each item.\nEigenvectors serve as a weight for each eigenvalue. The eigenvector \\(\\times\\) the square root of the eigenvalue = the component loading (which can be interpreted as the correlation of each item with the principal component). In other words, eigenvectors can be calculated as the component loading divided by the square root of the eigenvalue.\n\\[eigenvector = \\dfrac {component \\hspace1ex loading} {\\sqrt{ \\left( eigen \\hspace1ex value \\right)}} \\]\nFor this dataset, given that the eignevalue of the first item = 3.301 (check the “total variance explained” table) and the component loading for item 1 =0.637 (check the component matrix), we can then compute the eigenvector associated with the first item using the above formula\ncomponent loading = 0.637, eigenvalue = 3.301\n\\[eigenvector = \\dfrac {0.637} {\\sqrt{ \\left( 3.301 \\right)}} \\]\neigenvector of the first item = 0.35\nNote that the component loading = 0.637- Thus, we can say that the correlation of the first item with the first component is 0.637 (about 63.7%).\n\n\nComponent Matrix\nThe components can be seen as the correlation between each item and the respective component. Each item is associated with loadings for all 10 components. For instance, Item 1 is correlated at 0.637 with the first component, 0.182 with the second component, and -0.360 with the third, etc.\nThe square of each loading represents the proportion of variance explained (think of it as an \\(R^2\\) statistic) by a particular component. For Item 1, \\((0.637)^2\\)=0.406. This can be interpreted that 40.6% of its variance is explained by the first component. Also, \\((0.182)^2\\)=0.033 or 3.3% of the variance in Item 1 is explained by the second component. Note that the total variance explained by both components is thus 40.6%+3.3%=43.9%. When you accumulate these squared loadings progressively through the components, the total sums up to 1 or 100%. This is referred to as the communality, and in a PCA the communality for each item equates to the total variance.\n\n\n\n\n\nN.B: If you sum the squared component loadings across the components (columns), you’d get the communality estimates for each item. Also, if you sum each squared loading down the items (rows), you’d get the eigenvalue for each component.\nFor example, if you want to manually get the first eigenvalue, take a square of each item in column 1 and add them together- you’d get = 3.301 (the eigenvalue!). And if you’d do this for other columns, you’ll get ten eigenvalues for ten components- and that leads us to the next table.\n\n\nTotal Variance Explained\nRecall that the eigenvalue = overall variance that can be explained by a given principal component. Starting with the first component, each subsequent component is derived by factoring out the influence of the previous one. Therefore the first component explains the most amount of variation, and the last component explains the least. If you look at the “Total Variance Explained” table, you’ll find the total variance explained by each component. For instance, Component 1 = 3.301, or (3.301/10)%=33.01% of the total variance. Because we extracted the same number of components as there are items, the Initial Eigenvalues column should be the same as the Extraction Sums of Squared Loadings column.\n\n\n\n\n\n\n\nSelecting the optinal number of components to extract\nSince the goal of running a PCA is to reduce our set of variables down, it would be useful to have a criterion for selecting the optimal number of components that are of course smaller than the total number of items. One criterion is the choose components that have eigenvalues greater than 1. Under the Total Variance Explained table, we see the first two components have an eigenvalue greater than 1. This can be confirmed by the Scree Plot which plots the eigenvalue (total variance explained) by the component number.\nGiven that the aim of conducting a PCA is to reduce our variable set, it becomes valuable to establish a criterion for selecting the optimal number of components, which should ideally be fewer than the total number of items. One such criterion involves selecting components with eigenvalues &gt; 1. If you look at the “Total Variance Explained” table, you’d see that the first three components possess eigenvalues &gt; 1. We can also further confirm this by looking at the Scree Plot, which charts the eigenvalue (total variance explained) against the component number.\n\n\n\n\n\nRemember that we selected the Scree plot here- dimension reduction&gt;factor&gt;extraction&gt;scree plot. So, the scree plot should be produced automatically.\nThe first component consistently accounts for the highest total variance, while the last component will always explains the least. However, the main question is: where do we see the most significant drop in variance? For this example, if you look at component 3, it shows an “elbow” joint, signifying that it might not yield substantial benefits to continue extracting more components after this. There exist varied interpretations of the scree plot, but some suggest selecting the number of components to the left of this “elbow.” Following this criterion would lead us to opt for just two components. Another, more subjective perspective on the scree plot proposes that any number of components within the range of 1 to 4 could be valid, and additional supporting evidence would be beneficial.\nCertain criteria propose that the total variance explained by all components should fall within the range of 70% to 80% variance, which in this scenario would indicate about five to six components. However, as noted by Andy Field (the author of Discovering Statistics using IBM SPSS Statistics), this approach may be implausible in social science research, where extracted factors typically explain only 50% to 60% of the variance.\nChoosing the number of components involves a degree of subjectivity and ideally necessitates input from the entire research team. But for now, let’s suppose we consulted Dr. Steven Shaw, and he believes that a three-component solution aligns with the study’s objectives. Therefore, we can proceed with the analysis based on his input.\n\n\nPCA with 3 components\nTo run the three component PCA is just as easy as running the 10 component solution we did before. The only difference is under Fixed number of factors – Factors to extract you have to enter 3.\nFollow the same steps we did before\ngo to:\n\nAnalyze&gt; Dimension Reduction &gt;Factor. You can move all the observed variables over the Variables: box to be analyzed.\nClick on Extraction, then under the Method, select Principal components and make sure to Analyze the Correlation matrix.\nRequest the Unrotated factor solution and the Scree plot. Under Extract, choose Fixed number of factors, and under Factor to extract enter 3.\nIncrease the Maximum Iterations of Convergence to 100.\n\nNow, let’s focus on the differences in the output between the ten and three-component solution. If you look at the Total Variance Explained, we can see that the Initial Eigenvalues no longer equals the Extraction Sums of Squared Loadings. Specifically, The main difference is that there are only three rows of Extraction Sums of Squared Loadings, and the cumulative percent variance goes up to 56.671%.\n\n\n\n\n\nAlso, we can see that the Component Matrix has the same loadings as the ten-component solution but instead of ten columns it's now three columns.\n\n\n\n\n\nInterpretations- Again, we will interpret Item 1 as having a correlation of 0.637 with Component 1. From the table, we can see that q07 (All computers hate me) has the highest correlation with Component 1 and q02 (My friends will think I’m stupid for not being able to cope with SPSS) has the lowest. Also, we see that q9 has the highest correlation with Component 2 and q10 the lowest. Similarly, we can see that q06 has the highest with component 3 and q09 the lowest.\n\n\nTest:\nTrue or False\n\nThe elements of the Component Matrix are correlations of the item with each component.\nThe sum of the squared eigenvalues is the proportion of variance under Total Variance Explained.\nThe Component Matrix can be thought of as correlations and the Total Variance Explained table can be thought of as \\(R^2\\).\n\n\n\n\nCommunalities\nCommunality is calculated by summing the squares of the component loadings for the number of components you have extracted.\nLook at the SPSS output you will see a table of communalities.\n\n\n\n\n\nGiven that PCA is an iterative estimation process, it starts with 1 as an initial estimate of the communality (since this is the total variance across all 10 components), and then proceeds with the analysis until a final communality is extracted. Now, as a practice exercise, let's manually calculate the first communality from the Component Matrix. The first ordered is (0.637,0.182, -0.360) which represents the correlation of the first item with Component 1, 2, and 3.\n\\[h^2 = (0.637)^2 \\hspace1ex + \\hspace1ex (0.182)^2 \\hspace1ex + \\hspace1ex (-0.360)^2\\]\nIf you do this, you should get ~= 0.569 (extraction 1 under the communality table above).\nRecall that to get the communality, you should sqaure the loadings and sum down the components\nGoing back to the Communalities table, if you sum down all 10 items (rows) of the Extraction column, you get 5.667. Also, If you go back to the Total Variance Explained table and summed the first three eigenvalues you also get 3.301+1.333+1.033=5.667. Is that surprising? Basically this implies that summing the communalities across all items is the same as summing the eigenvalues across all components.\n\nTest\n1. In a PCA, when would the communality for the Initial column be equal to the Extraction column?\nHint- When you run a full component PCA as the number of items. In this case, 10-component PCA.\nTrue or False\n\nThe eigenvalue represents the communality for each item.\nFor a single component, the sum of squared component loadings across all items represents the eigenvalue for that component.\nThe sum of eigenvalues for all the components is the total variance.\nThe sum of the communalities down the components is equal to the sum of eigenvalues down the items.\n\nSolutions:\n1. F, the eigenvalue = total communality across all items for a single component, 2. T, 3. T, 4. F (we sum communalities across items, and sum eigenvalues across components, which should be equal- we just did that now).\n\n\n\nCommon Factor Analysis\nThe difference between principal components analysis (PCA) and common factor analysis lies in how they partition variance. Both methods are used for dimensionality reduction of the dataset to a smaller number of unobserved variables. However, PCA assumes that common variances account for all the total variance, while common factor analysis presumes that total variance can be divided into common and unique variance. The latter assumption is often more realistic, acknowledging that the measurement of the item set may not be perfect. The unobserved or latent variable responsible for common variance is termed a factor, hence the name “factor analysis.”\nAnother significant difference between PCA and factor analysis pertains to the objective of the analysis. If the goal is simply to reduce the list of variables into a linear combination of smaller components, you can just run a PCA. However, if there is a prior knowledge that a latent construct underlies the interrelationships among the items, factor analysis may be more suitable.\nIn our example, we assume the existence of a construct (“SPSS Anxiety”) that explains the correlations among all items in the SAQ10. However, it’s recognized that SPSS Anxiety cannot account for all the shared variance among SAQ items, so we can also model the unique variance.\nFrom the PCA result above, we can run a three-factor extraction.\n\nRun a Common Factor Analysis with 3 factors in SPSS\nTo do this in SPSS, use the same steps as running a PCA above-Analyze&gt; Dimension Reduction &gt; Factor. Select “Extraction” and under Method choose Principal axis factoring. Let’s keep the Maximum Iterations for Convergence at 100 and run the analysis.\nNote the main difference is under the EXTRACTION METHOD- here, we see PAF (Principal Axis Factoring) instead of PC (Principal Components). We will get three tables and 1 scree plot as output, Communalities, Total Variance Explained and Factor Matrix.\nLet's go over each of these and compare them to the PCA output.\nCommunalities of the PAF\n\n\n\n\n\nFrom this table, the most obvious difference between this communalities and the one we did earlier for PCA is that the initial extraction is no longer 1. Remember that for a PCA, we assume the total variance = common variance or communality (so we pick 1 as our best initial guess). However, for principal axis factoring, instead of guessing 1 as the initial communality, it chooses the squared multiple correlation coefficient \\(R^2\\).\nLet’s see this in action- at least for first item in the list  We can do this by running a linear regression where q01 is the response variable and Items 2 -10 are predictors.\n\n\nTest\nRUn a linear regression in SPSS-\nHint- Analyze &gt; Regression &gt;Linear and enter q01 under Dependent and q02 to q10 under Independent(s).\nYou should get output similar to the one below\n\n\n\n\n\nNote that our \\(R^2\\) here = 0.295 and that matches the initial communality estimate for q01 (see the communality table above). Equally, we can do the remaining 9 linear regressions in order to get all ten communality estimates but we don’t have to since SPSS did that already (the communality table). Similar to PCA,  factor analysis also utilizes an iterative estimation process to obtain the final estimates under the Extraction column. Finally, if we sum all the rows of the extraction column, and we get 3.98. This represents the total common variance shared among all items in our 3-factor analysis.\n\n\n\nTotal Variance Explained\nLet’s check the Total Variance Explained table. If we compare this to the table from the PCA, we can see that the Initial Eigenvalues are the same and includes 10 rows for each \"factor\".\n\n\n\n\n\nIndeed, SPSS leverages the information obtained from the PCA analysis for use in the factor analysis. The factors used in factor analysis are essentially the components found in the “Initial Eigenvalues” column. The primary distinction lies in the “Extraction Sums of Squared Loadings” column. Here, each corresponding entry in the Extraction column is lower than the Initial column. This outcome aligns with our expectations, as we assume that the total variance can be divided into common and unique variance. Consequently, the common variance explained will be relatively reduced. From the result, we can see that Factor 1 accounts for 27.342% of the variance, Factor 2 explains 6.889% of the variance, while Factor 3 explains 5.543% of the variance. Much like in PCA, the more factors you extract, the less variance is explained by each successive factor.\n\nTest\nIn theory, when would the percent of variance in the Initial column ever equal the Extraction column?\n(Hint- When there is no unique variance- recall that PCA assumes this whereas common factor analysis does not, so this is in theory and not in practice)\n\n\n\nFactor Matrix\nIf you remember, we set the number of iteration to 100. Like I said before, this is necessary to reach convergence. Look at the table, you’d see that 49 iterations were required. If we had simply used the default 25 iterations in SPSS, we would not have reached convergence or obtained an optimal solution. Hence, in practice, it's always good to increase the maximum number of iterations.\n\n\n\n\n\nFrom the table, the elements of the Factor Matrix table are called loadings and denotes the correlation of each item with the corresponding factor. Similar to PCA, if we square each loading and sum down the items (rows), we would get the total variance explained by each factor. N.B.: they are no longer called eigenvalues as in PCA but Sums of Squared Loadings.\nLet's calculate this for Factor 1:\n\\[(0.575)^2 + (-0.280)^2 + (-0.602)^2 + (0.640)^2 + (0.561)^2 + (0.596)^2 + (0.662)^2 + (0.456)^2 + (-0.272)^2 + (0.405)^2\\]\n\\(== 2.73\\) which matches the first row under the Extraction column of the Total Variance Explained table. We can repeat this for Factor 2 and get the same results for the second row (0.689).\nAlso, we can get the communality estimates by adding the squared loadings across the factors (columns) for each item. For example, for q01:\n\\((0.575)^2 + (0.60)^2 + (0.298)^2 = 0.423\\)\nN.B.: these results match the value of the Communalities table for q01 (see the Extraction column). This means that the sum of squared loadings across factors \\(=\\) communality estimates for each item. You can do the same for all other items on the list and you’d get the same result as in “Extraction” of the communality table.\n\n\nThe relationship between the three tables\nTo understand the relationships among the three tables, let’s begin with the Factor Matrix (referred to as the Component Matrix in PCA). We’ll use the term “factor” to describe components, applicable to PCA as well. Within this matrix, you’ll find the elements that signify the correlation between each item and each respective factor.\nNow, when you square each of these elements, you obtain squared loadings, which represent the proportion of variance explained by each factor for each item. The cumulative sum of these squared loadings across factors provides the proportion of variance explained collectively by all the factors in the model. This is known as common variance or communality, and it’s the outcome displayed in the Communalities table.\nFor the Factor Matrix, if you square the loadings and sum them down the items, you get the Sums of Squared Loadings (in PAF) or eigenvalues (in PCA) for each factor. These values now become the elements within the Total Variance Explained table.\nWhen you sum down the rows, which is to say, you add up the values along each factor, under the Extraction column, you arrive at a total, let’s say, \\(2.734 + 0.689 + 0.554 = 3.977\\). This value represents the overall (common) variance explained by the three-factor solution for all ten items. Equivalently, because the Communalities table represents the total common variance accounted for by both factors for each item, adding up the values down the items in the Communalities table similarly yields the total (common) variance explained.\n\\((0.423)+(0.308)+(0.465)+(0.465)+(0.347)+(0.709)+(0.477)+(0.271) + (0.331) + (0.182)=3.978\\) -&gt; the same result we obtained from the Total Variance Explained table.\nTo summarize:\n\nBy squaring the elements within the Factor Matrix, you get the squared loadings.\nThe summation of the squared loadings from the Factor Matrix across the factors provides the communality estimates for each item, as found in the Extraction column of the Communalities table.\nSumming the squared loadings within the Factor Matrix along the items yields the Sums of Squared Loadings (in PAF) or eigenvalues (in PCA) for each factor across all items.\nWhen you add up the eigenvalues or Sums of Squared Loadings in the Total Variance Explained table, you get the total common variance explained.\nAdding up all the items in the Communalities table is equivalent to summing the eigenvalues or Sums of Squared Loadings down all the factors, which can be found under the Extraction column of the Total Variance Explained table.\n\n\nTest\nTrue or False (the following assumes a three-factor Principal Axis Factor solution with 10 items)\n\nThe entries within the Factor Matrix represent the correlations between each item and a factor.\nEach squared element in the Factor Matrix for Item 1 represents the communality.\nThe summation of these squared entries in the Factor Matrix across all 10 items within Factor 1 corresponds to the first Sums of Squared Loading in the Extraction column of the Total Variance Explained table.\nAdding all 10 items in the Extraction column of the Communalities table provides the total common variance explained by both factors.\nThe total common variance explained is calculated by summing all Sums of Squared Loading values in the Initial column of the Total Variance Explained table\nThe cumulative Sums of Squared Loadings in the Extraction column under the Total Variance Explained table represents the total variance,comprising both total common variance and unique variance\nIn common factor analysis, the sum of squared loadings is referred to as the eigenvalue.\n\nHints- 1, 3,4 T, 2. F, it’s the sum of the squared elements across factors, 5. F, sum all eigenvalues from the Extraction column of the Total Variance Explained table, 6. F, the total Sums of Squared Loadings represents only the total common variance excluding unique variance, 7. F, eigenvalues are only applicable for PCA."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to EDUC 806. This website will be used for all the tutorials of this course. For this course, we will be using SPSS throughout the semester. However, anyone interested in using R (or Python) should feel free to do so (and I’m happy to chat about that). If you’re generally interested in using R for statistical analysis and would like a quick intro, please refer to this website R Workshop for Statistical Analysis.\nIn fact, this website was built entirely using R!"
  }
]